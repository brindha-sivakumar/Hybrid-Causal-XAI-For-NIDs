{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1b715",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Hybrid Explanation Generator - USING SHAP\n",
    "# Key changes from original:\n",
    "# 1. Replace DeepLIFT with SHAP KernelExplainer\n",
    "# 2. Use background data for SHAP baseline\n",
    "# 3. Extract class 1 (Important) attributions\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 4: HYBRID EXPLANATION GENERATOR (USING SHAP)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "LSTM_MODEL_PATH = '../step1_lstm_xai/best_lstm.pt'\n",
    "SCALER_PATH = '../step1_lstm_xai/scaler.joblib'\n",
    "CAUSAL_GRAPH_PATH = '../step2_causal_discovery/causal_graph.gpickle'\n",
    "DATA_PATH = '../UNSW_NB15_training-set.csv'\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    'dur', 'proto', 'service', 'state', 'spkts', 'dpkts',\n",
    "    'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload',\n",
    "    'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit',\n",
    "    'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat',\n",
    "    'smean', 'dmean', 'trans_depth', 'response_body_len',\n",
    "    'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm',\n",
    "    'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login',\n",
    "    'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst',\n",
    "    'is_sm_ips_ports'\n",
    "]\n",
    "\n",
    "CAUSAL_FEATURES = [\n",
    "    'proto', 'sttl', 'state', 'dtcpb', 'is_sm_ips_ports',\n",
    "    'dttl', 'stcpb', 'service', 'dwin', 'swin'\n",
    "]\n",
    "\n",
    "MISSING_VALUE_INDICATOR = -1.0\n",
    "\n",
    "# ==================== LOAD MODELS AND DATA ====================\n",
    "print(\"\\nLoading models and data...\")\n",
    "\n",
    "# Load LSTM model architecture\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "# Load trained model\n",
    "device = torch.device('cpu')\n",
    "input_size = len(FEATURE_NAMES)\n",
    "model = LSTMClassifier(input_size=input_size).to(device)\n",
    "\n",
    "if Path(LSTM_MODEL_PATH).exists():\n",
    "    model.load_state_dict(torch.load(LSTM_MODEL_PATH, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"✓ Loaded LSTM model from {LSTM_MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"⚠ Warning: {LSTM_MODEL_PATH} not found. Using untrained model.\")\n",
    "\n",
    "# Load scaler\n",
    "if Path(SCALER_PATH).exists():\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    print(f\"✓ Loaded scaler from {SCALER_PATH}\")\n",
    "else:\n",
    "    print(f\"⚠ Warning: {SCALER_PATH} not found.\")\n",
    "    scaler = None\n",
    "\n",
    "# Load causal graph\n",
    "if Path(CAUSAL_GRAPH_PATH).exists():\n",
    "    causal_graph = pickle.load(open(CAUSAL_GRAPH_PATH, 'rb'))\n",
    "    print(f\"✓ Loaded causal graph from {CAUSAL_GRAPH_PATH}\")\n",
    "else:\n",
    "    print(f\"⚠ Warning: {CAUSAL_GRAPH_PATH} not found.\")\n",
    "    causal_graph = nx.DiGraph()\n",
    "\n",
    "# Load data\n",
    "if Path(DATA_PATH).exists():\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"✓ Loaded data from {DATA_PATH}: {df.shape}\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    drop_cols = ['id', 'attack_cat']\n",
    "    for col in drop_cols:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "    \n",
    "    if 'label' in df.columns and 'Label' not in df.columns:\n",
    "        df['Label'] = df['label']\n",
    "    \n",
    "    categorical_cols = ['proto', 'service', 'state']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns and df[col].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "    \n",
    "    print(\"✓ Categorical features encoded\")\n",
    "else:\n",
    "    print(f\"⚠ Warning: {DATA_PATH} not found.\")\n",
    "    df = None\n",
    "\n",
    "# ==================== SHAP EXPLAINER SETUP ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"XAI COMPONENT: SHAP Explainer\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "shap_explainer = None\n",
    "background_data = None\n",
    "\n",
    "if df is not None and scaler is not None:\n",
    "    try:\n",
    "        import shap\n",
    "        \n",
    "        # Get background data (sample from training data)\n",
    "        print(\"\\nInitializing SHAP explainer...\")\n",
    "        available_features = [f for f in FEATURE_NAMES if f in df.columns]\n",
    "        \n",
    "        if len(available_features) == len(FEATURE_NAMES):\n",
    "            # Sample 100 background samples\n",
    "            background_indices = np.random.choice(len(df), size=min(100, len(df)), replace=False)\n",
    "            background_data_raw = df.iloc[background_indices][FEATURE_NAMES].values\n",
    "            \n",
    "            # Scale background data\n",
    "            background_data = scaler.transform(background_data_raw)\n",
    "            print(f\"  Background data shape: {background_data.shape}\")\n",
    "            \n",
    "            # Define model prediction function for SHAP\n",
    "            def model_predict_shap(x):\n",
    "                \"\"\"\n",
    "                Prediction function for SHAP\n",
    "                Input: numpy array (n_samples, n_features) - SCALED\n",
    "                Output: numpy array (n_samples, n_classes) - probabilities\n",
    "                \"\"\"\n",
    "                if x.ndim == 2:\n",
    "                    # Add sequence dimension: (batch, features) -> (batch, 1, features)\n",
    "                    x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                else:\n",
    "                    x_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output = model(x_tensor)\n",
    "                    probs = torch.softmax(output, dim=1).cpu().numpy()\n",
    "                \n",
    "                return probs\n",
    "            \n",
    "            # Initialize KernelExplainer\n",
    "            shap_explainer = shap.KernelExplainer(\n",
    "                model_predict_shap,\n",
    "                background_data\n",
    "            )\n",
    "            print(\"✓ SHAP KernelExplainer initialized\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"⚠ Warning: Missing features. SHAP unavailable.\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"⚠ Warning: SHAP not installed. Install with: pip install shap\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: SHAP initialization failed: {e}\")\n",
    "\n",
    "# ==================== XAI COMPONENT WITH SHAP ====================\n",
    "\n",
    "def compute_shap_attribution(shap_explainer, alert_tensor_scaled, n_samples=100):\n",
    "    \"\"\"\n",
    "    Compute SHAP attributions for a single alert\n",
    "    \n",
    "    Args:\n",
    "        shap_explainer: SHAP KernelExplainer instance\n",
    "        alert_tensor_scaled: numpy array (n_features,) - MUST BE SCALED\n",
    "        n_samples: Number of samples for SHAP approximation\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of feature attributions for class 1 (Important)\n",
    "    \"\"\"\n",
    "    if shap_explainer is None:\n",
    "        print(\"    ⚠ SHAP explainer not available, using gradient fallback\")\n",
    "        return compute_gradient_attribution_fallback(alert_tensor_scaled)\n",
    "    \n",
    "    try:\n",
    "        # SHAP expects 2D input: (1, n_features)\n",
    "        alert_2d = alert_tensor_scaled.reshape(1, -1)\n",
    "        \n",
    "        # Compute SHAP values\n",
    "        shap_values = shap_explainer.shap_values(alert_2d, nsamples=n_samples)\n",
    "        \n",
    "        # Debug: Check shape\n",
    "        print(f\"    [SHAP DEBUG] Raw SHAP output type: {type(shap_values)}\")\n",
    "        if isinstance(shap_values, list):\n",
    "            print(f\"    [SHAP DEBUG] List length: {len(shap_values)}\")\n",
    "            for i, sv in enumerate(shap_values):\n",
    "                print(f\"    [SHAP DEBUG] Class {i} shape: {sv.shape if hasattr(sv, 'shape') else type(sv)}\")\n",
    "        else:\n",
    "            print(f\"    [SHAP DEBUG] Single output shape: {shap_values.shape}\")\n",
    "        \n",
    "        # Extract class 1 (Important) attributions\n",
    "        if isinstance(shap_values, list):\n",
    "            # Multi-class output: [class_0_shap, class_1_shap]\n",
    "            attributions = shap_values[1]  # Get class 1\n",
    "            \n",
    "            # Handle different dimensionalities\n",
    "            if attributions.ndim > 1:\n",
    "                attributions = attributions.flatten()  # Flatten to 1D\n",
    "        else:\n",
    "            # Single output (binary classification)\n",
    "            attributions = shap_values\n",
    "            \n",
    "            # Handle different dimensionalities\n",
    "            if attributions.ndim > 1:\n",
    "                attributions = attributions.flatten()  # Flatten to 1D\n",
    "        \n",
    "        print(f\"    [SHAP DEBUG] Final attribution shape: {attributions.shape}\")\n",
    "        \n",
    "        return attributions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠ SHAP failed: {e}. Using gradient fallback.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return compute_gradient_attribution_fallback(alert_tensor_scaled)\n",
    "\n",
    "def compute_gradient_attribution_fallback(alert_features_scaled):\n",
    "    \"\"\"\n",
    "    Fallback: Simple gradient-based attribution\n",
    "    \"\"\"\n",
    "    alert_tensor = torch.tensor(alert_features_scaled, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    alert_tensor.requires_grad = True\n",
    "    \n",
    "    output = model(alert_tensor)\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "    \n",
    "    model.zero_grad()\n",
    "    output[0, pred_class].backward()\n",
    "    \n",
    "    gradients = alert_tensor.grad.squeeze().detach().cpu().numpy()\n",
    "    values = alert_tensor.squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    attributions = gradients * values\n",
    "    return attributions\n",
    "\n",
    "def is_missing_value(value, threshold=MISSING_VALUE_INDICATOR):\n",
    "    \"\"\"Check if a feature value represents missing/NA data\"\"\"\n",
    "    return abs(value - threshold) < 1e-6\n",
    "\n",
    "def generate_xai_explanation(model, alert_features, feature_names, top_k=5, scaler=None, shap_explainer=None):\n",
    "    \"\"\"\n",
    "    Generate XAI explanation using SHAP\n",
    "    \n",
    "    Args:\n",
    "        model: LSTM model\n",
    "        alert_features: numpy array of feature values (UNSCALED)\n",
    "        feature_names: list of feature names\n",
    "        top_k: number of top features\n",
    "        scaler: MinMaxScaler\n",
    "        shap_explainer: SHAP KernelExplainer instance\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with XAI results\n",
    "    \"\"\"\n",
    "    # Scale features\n",
    "    if scaler is not None:\n",
    "        alert_features_scaled = scaler.transform(alert_features.reshape(1, -1)).flatten()\n",
    "    else:\n",
    "        alert_features_scaled = alert_features\n",
    "    \n",
    "    # Get prediction\n",
    "    alert_tensor = torch.tensor(alert_features_scaled, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(alert_tensor)\n",
    "        probs = torch.softmax(output, dim=1)[0]\n",
    "        pred_class = output.argmax(dim=1).item()\n",
    "        confidence = probs[pred_class].item()\n",
    "    \n",
    "    print(f\"  [DEBUG] Prediction: {pred_class}, Confidence: {confidence:.4f}\")\n",
    "    \n",
    "    # Compute SHAP attributions\n",
    "    attributions = compute_shap_attribution(shap_explainer, alert_features_scaled, n_samples=50)\n",
    "    \n",
    "    print(f\"  [DEBUG] Attribution range: [{attributions.min():.4f}, {attributions.max():.4f}]\")\n",
    "    \n",
    "    # Combine features with attributions\n",
    "    feature_importance = []\n",
    "    for name, attr, value in zip(feature_names, attributions, alert_features):\n",
    "        feature_importance.append({\n",
    "            'feature': name,\n",
    "            'importance': float(attr),\n",
    "            'value': float(value),\n",
    "            'abs_importance': float(abs(attr)),\n",
    "            'is_missing': is_missing_value(value)\n",
    "        })\n",
    "    \n",
    "    # Sort by absolute importance\n",
    "    feature_importance.sort(key=lambda x: x['abs_importance'], reverse=True)\n",
    "    \n",
    "    # Filter out missing values\n",
    "    feature_importance_present = [f for f in feature_importance if not f['is_missing']]\n",
    "    \n",
    "    missing_count = len([f for f in feature_importance if f['is_missing']])\n",
    "    present_count = len(feature_importance_present)\n",
    "    \n",
    "    print(f\"  [DEBUG] Features: {present_count} present, {missing_count} missing\")\n",
    "    \n",
    "    top_features = feature_importance_present[:top_k] if len(feature_importance_present) >= top_k else feature_importance[:top_k]\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'Important' if pred_class == 1 else 'Irrelevant',\n",
    "        'confidence': confidence,\n",
    "        'pred_class': pred_class,\n",
    "        'top_features': top_features,\n",
    "        'all_features': feature_importance,\n",
    "        'num_missing_features': missing_count,\n",
    "        'num_present_features': present_count\n",
    "    }\n",
    "\n",
    "# ==================== CAUSAL COMPONENT ====================\n",
    "# [Keep all causal analysis functions from original code]\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CAUSAL COMPONENT: Root Cause Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def find_root_causes(graph, target_feature):\n",
    "    \"\"\"Find all root causes of target\"\"\"\n",
    "    if target_feature not in graph:\n",
    "        return []\n",
    "    ancestors = nx.ancestors(graph, target_feature)\n",
    "    return [node for node in ancestors if graph.in_degree(node) == 0]\n",
    "\n",
    "def find_causal_path(graph, source, target):\n",
    "    \"\"\"Find shortest causal path\"\"\"\n",
    "    if source not in graph or target not in graph:\n",
    "        return None\n",
    "    try:\n",
    "        return nx.shortest_path(graph, source, target)\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "def get_direct_causes(graph, feature):\n",
    "    \"\"\"Get direct causes (parents)\"\"\"\n",
    "    if feature not in graph:\n",
    "        return []\n",
    "    return list(graph.predecessors(feature))\n",
    "\n",
    "def analyze_causal_chain(graph, target_feature, alert_data, all_feature_names):\n",
    "    \"\"\"Analyze causal chains leading to target\"\"\"\n",
    "    if target_feature not in graph:\n",
    "        return {\n",
    "            'target': target_feature,\n",
    "            'in_graph': False,\n",
    "            'root_causes': [],\n",
    "            'causal_paths': [],\n",
    "            'direct_causes': [],\n",
    "            'reason': 'Feature not in causal graph'\n",
    "        }\n",
    "    \n",
    "    root_causes = find_root_causes(graph, target_feature)\n",
    "    \n",
    "    causal_paths = []\n",
    "    for root in root_causes:\n",
    "        path = find_causal_path(graph, root, target_feature)\n",
    "        if path:\n",
    "            path_with_values = [\n",
    "                {'feature': f, 'value': alert_data.get(f, 'N/A')}\n",
    "                for f in path\n",
    "            ]\n",
    "            causal_paths.append({\n",
    "                'root': root,\n",
    "                'path': path,\n",
    "                'path_with_values': path_with_values,\n",
    "                'length': len(path)\n",
    "            })\n",
    "    \n",
    "    direct_causes = get_direct_causes(graph, target_feature)\n",
    "    direct_causes_with_values = [\n",
    "        {'feature': c, 'value': alert_data.get(c, 'N/A')}\n",
    "        for c in direct_causes\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'target': target_feature,\n",
    "        'in_graph': True,\n",
    "        'root_causes': root_causes,\n",
    "        'causal_paths': causal_paths,\n",
    "        'direct_causes': direct_causes_with_values,\n",
    "        'num_paths': len(causal_paths)\n",
    "    }\n",
    "\n",
    "# ==================== HYBRID EXPLAINER ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYBRID EXPLAINER: Combining SHAP + Causal\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class HybridExplainer:\n",
    "    \"\"\"Combines SHAP and Causal Analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, model, causal_graph, feature_names, scaler=None, shap_explainer=None):\n",
    "        self.model = model\n",
    "        self.graph = causal_graph\n",
    "        self.feature_names = feature_names\n",
    "        self.scaler = scaler\n",
    "        self.shap_explainer = shap_explainer\n",
    "    \n",
    "    def explain(self, alert_data, alert_id=None):\n",
    "        \"\"\"Generate hybrid explanation\"\"\"\n",
    "        if isinstance(alert_data, dict):\n",
    "            alert_features = np.array([alert_data.get(f, 0) for f in self.feature_names])\n",
    "            alert_dict = alert_data\n",
    "        else:\n",
    "            alert_features = alert_data\n",
    "            alert_dict = {f: v for f, v in zip(self.feature_names, alert_features)}\n",
    "        \n",
    "        # Get XAI explanation using SHAP\n",
    "        xai_results = generate_xai_explanation(\n",
    "            self.model,\n",
    "            alert_features,\n",
    "            self.feature_names,\n",
    "            top_k=5,\n",
    "            scaler=self.scaler,\n",
    "            shap_explainer=self.shap_explainer\n",
    "        )\n",
    "        \n",
    "        # Analyze causal chains\n",
    "        causal_analyses = []\n",
    "        for feat_info in xai_results['top_features']:\n",
    "            feature_name = feat_info['feature']\n",
    "            if feature_name in self.graph:\n",
    "                causal_analysis = analyze_causal_chain(\n",
    "                    self.graph, feature_name, alert_dict, self.feature_names\n",
    "                )\n",
    "            else:\n",
    "                causal_analysis = {\n",
    "                    'target': feature_name,\n",
    "                    'in_graph': False,\n",
    "                    'root_causes': [],\n",
    "                    'causal_paths': [],\n",
    "                    'direct_causes': [],\n",
    "                    'reason': 'Feature not in causal graph'\n",
    "                }\n",
    "            causal_analyses.append(causal_analysis)\n",
    "        \n",
    "        # Label causal analysis\n",
    "        label_causal = None\n",
    "        if 'Label' in self.graph:\n",
    "            label_causal = analyze_causal_chain(\n",
    "                self.graph, 'Label', alert_dict, self.feature_names\n",
    "            )\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = self._generate_recommendations(\n",
    "            xai_results, causal_analyses, alert_dict\n",
    "        )\n",
    "        \n",
    "        # Keep the rest of HybridExplainer and HybridExplanation classes\n",
    "        # from original code...\n",
    "        \n",
    "        return {\n",
    "            'alert_id': alert_id,\n",
    "            'xai': xai_results,\n",
    "            'causal': causal_analyses,\n",
    "            'label_causal': label_causal,\n",
    "            'recommendations': recommendations\n",
    "        }\n",
    "    \n",
    "    def _generate_recommendations(self, xai_results, causal_analyses, alert_data):\n",
    "        \"\"\"Generate recommendations (keep from original)\"\"\"\n",
    "        # [Keep full _generate_recommendations method from original code]\n",
    "        pass\n",
    "\n",
    "# ==================== DEMO ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING DEMO EXPLANATIONS WITH SHAP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "explainer = HybridExplainer(\n",
    "    model=model,\n",
    "    causal_graph=causal_graph,\n",
    "    feature_names=FEATURE_NAMES,\n",
    "    scaler=scaler,\n",
    "    shap_explainer=shap_explainer\n",
    ")\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\nGenerating explanations for sample alerts...\")\n",
    "    sample_indices = [0, 100, 500]\n",
    "    \n",
    "    for idx in sample_indices[:1]:  # Test with one sample first\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EXAMPLE ALERT #{idx}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        alert_row = df.iloc[idx]\n",
    "        alert_features = alert_row[FEATURE_NAMES].values\n",
    "        \n",
    "        explanation = explainer.explain(alert_features, alert_id=idx)\n",
    "        \n",
    "        print(\"\\nPrediction:\", explanation['xai']['prediction'])\n",
    "        print(\"Confidence:\", f\"{explanation['xai']['confidence']:.1%}\")\n",
    "        print(\"\\nTop Features (SHAP):\")\n",
    "        for i, feat in enumerate(explanation['xai']['top_features'], 1):\n",
    "            print(f\"  {i}. {feat['feature']}: {feat['importance']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4 COMPLETE - USING SHAP\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
