{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1b715",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Hybrid Explanation Generator - USING SHAP\n",
    "# Key changes from original:\n",
    "# 1. Replace DeepLIFT with SHAP KernelExplainer\n",
    "# 2. Use background data for SHAP baseline\n",
    "# 3. Extract class 1 (Important) attributions\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 4: HYBRID EXPLANATION GENERATOR (USING SHAP)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "LSTM_MODEL_PATH = '../step1_lstm_xai/best_lstm.pt'\n",
    "SCALER_PATH = '../step1_lstm_xai/scaler.joblib'\n",
    "CAUSAL_GRAPH_PATH = '../step2_causal_discovery/causal_graph.gpickle'\n",
    "DATA_PATH = '../UNSW_NB15_training-set.csv'\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    'dur', 'proto', 'service', 'state', 'spkts', 'dpkts',\n",
    "    'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload',\n",
    "    'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit',\n",
    "    'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat',\n",
    "    'smean', 'dmean', 'trans_depth', 'response_body_len',\n",
    "    'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm',\n",
    "    'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login',\n",
    "    'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst',\n",
    "    'is_sm_ips_ports'\n",
    "]\n",
    "\n",
    "CAUSAL_FEATURES = [\n",
    "    'proto', 'sttl', 'state', 'dtcpb', 'is_sm_ips_ports',\n",
    "    'dttl', 'stcpb', 'service', 'dwin', 'swin'\n",
    "]\n",
    "\n",
    "MISSING_VALUE_INDICATOR = -1.0\n",
    "\n",
    "# ==================== LOAD MODELS AND DATA ====================\n",
    "print(\"\\nLoading models and data...\")\n",
    "\n",
    "# Load LSTM model architecture\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "# Load trained model\n",
    "device = torch.device('cpu')\n",
    "input_size = len(FEATURE_NAMES)\n",
    "model = LSTMClassifier(input_size=input_size).to(device)\n",
    "\n",
    "if Path(LSTM_MODEL_PATH).exists():\n",
    "    model.load_state_dict(torch.load(LSTM_MODEL_PATH, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"âœ“ Loaded LSTM model from {LSTM_MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"âš  Warning: {LSTM_MODEL_PATH} not found. Using untrained model.\")\n",
    "\n",
    "# Load scaler\n",
    "if Path(SCALER_PATH).exists():\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    print(f\"âœ“ Loaded scaler from {SCALER_PATH}\")\n",
    "else:\n",
    "    print(f\"âš  Warning: {SCALER_PATH} not found.\")\n",
    "    scaler = None\n",
    "\n",
    "# Load causal graph\n",
    "if Path(CAUSAL_GRAPH_PATH).exists():\n",
    "    causal_graph = pickle.load(open(CAUSAL_GRAPH_PATH, 'rb'))\n",
    "    print(f\"âœ“ Loaded causal graph from {CAUSAL_GRAPH_PATH}\")\n",
    "else:\n",
    "    print(f\"âš  Warning: {CAUSAL_GRAPH_PATH} not found.\")\n",
    "    causal_graph = nx.DiGraph()\n",
    "\n",
    "# Load data\n",
    "if Path(DATA_PATH).exists():\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"âœ“ Loaded data from {DATA_PATH}: {df.shape}\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    drop_cols = ['id', 'attack_cat']\n",
    "    for col in drop_cols:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "    \n",
    "    if 'label' in df.columns and 'Label' not in df.columns:\n",
    "        df['Label'] = df['label']\n",
    "    \n",
    "    categorical_cols = ['proto', 'service', 'state']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns and df[col].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "    \n",
    "    print(\"âœ“ Categorical features encoded\")\n",
    "else:\n",
    "    print(f\"âš  Warning: {DATA_PATH} not found.\")\n",
    "    df = None\n",
    "\n",
    "# ==================== SHAP EXPLAINER SETUP ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"XAI COMPONENT: SHAP Explainer\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "shap_explainer = None\n",
    "background_data = None\n",
    "\n",
    "if df is not None and scaler is not None:\n",
    "    try:\n",
    "        import shap\n",
    "        \n",
    "        # Get background data (sample from training data)\n",
    "        print(\"\\nInitializing SHAP explainer...\")\n",
    "        available_features = [f for f in FEATURE_NAMES if f in df.columns]\n",
    "        \n",
    "        if len(available_features) == len(FEATURE_NAMES):\n",
    "            # Sample 100 background samples\n",
    "            background_indices = np.random.choice(len(df), size=min(100, len(df)), replace=False)\n",
    "            background_data_raw = df.iloc[background_indices][FEATURE_NAMES].values\n",
    "            \n",
    "            # Scale background data\n",
    "            background_data = scaler.transform(background_data_raw)\n",
    "            print(f\"  Background data shape: {background_data.shape}\")\n",
    "            \n",
    "            # Define model prediction function for SHAP\n",
    "            def model_predict_shap(x):\n",
    "                \"\"\"\n",
    "                Prediction function for SHAP\n",
    "                Input: numpy array (n_samples, n_features) - SCALED\n",
    "                Output: numpy array (n_samples, n_classes) - probabilities\n",
    "                \"\"\"\n",
    "                if x.ndim == 2:\n",
    "                    # Add sequence dimension: (batch, features) -> (batch, 1, features)\n",
    "                    x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                else:\n",
    "                    x_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output = model(x_tensor)\n",
    "                    probs = torch.softmax(output, dim=1).cpu().numpy()\n",
    "                \n",
    "                return probs\n",
    "            \n",
    "            # Initialize KernelExplainer\n",
    "            shap_explainer = shap.KernelExplainer(\n",
    "                model_predict_shap,\n",
    "                background_data\n",
    "            )\n",
    "            print(\"âœ“ SHAP KernelExplainer initialized\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"âš  Warning: Missing features. SHAP unavailable.\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"âš  Warning: SHAP not installed. Install with: pip install shap\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Warning: SHAP initialization failed: {e}\")\n",
    "\n",
    "# ==================== XAI COMPONENT WITH SHAP ====================\n",
    "\n",
    "def compute_shap_attribution(shap_explainer, alert_tensor_scaled, n_samples=100):\n",
    "    \"\"\"\n",
    "    Compute SHAP attributions for a single alert\n",
    "    \n",
    "    Args:\n",
    "        shap_explainer: SHAP KernelExplainer instance\n",
    "        alert_tensor_scaled: numpy array (n_features,) - MUST BE SCALED\n",
    "        n_samples: Number of samples for SHAP approximation\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of feature attributions for class 1 (Important)\n",
    "    \"\"\"\n",
    "    if shap_explainer is None:\n",
    "        print(\"    âš  SHAP explainer not available, using gradient fallback\")\n",
    "        return compute_gradient_attribution_fallback(alert_tensor_scaled)\n",
    "    \n",
    "    try:\n",
    "        # SHAP expects 2D input: (1, n_features)\n",
    "        alert_2d = alert_tensor_scaled.reshape(1, -1)\n",
    "        \n",
    "        # Compute SHAP values\n",
    "        shap_values = shap_explainer.shap_values(alert_2d, nsamples=n_samples)\n",
    "        \n",
    "        # Debug: Check shape\n",
    "        print(f\"    [SHAP DEBUG] Raw SHAP output type: {type(shap_values)}\")\n",
    "        if isinstance(shap_values, list):\n",
    "            print(f\"    [SHAP DEBUG] List length: {len(shap_values)}\")\n",
    "            for i, sv in enumerate(shap_values):\n",
    "                print(f\"    [SHAP DEBUG] Class {i} shape: {sv.shape if hasattr(sv, 'shape') else type(sv)}\")\n",
    "        else:\n",
    "            print(f\"    [SHAP DEBUG] Single output shape: {shap_values.shape}\")\n",
    "        \n",
    "        # Extract class 1 (Important) attributions\n",
    "        if isinstance(shap_values, list):\n",
    "            # Multi-class output: [class_0_shap, class_1_shap]\n",
    "            attributions = shap_values[1]  # Get class 1\n",
    "            \n",
    "            # Handle different dimensionalities\n",
    "            if attributions.ndim > 1:\n",
    "                attributions = attributions.flatten()  # Flatten to 1D\n",
    "        else:\n",
    "            # Single output array from KernelExplainer\n",
    "            # Shape can be: (1, n_features) or (1, n_features, n_classes)\n",
    "            \n",
    "            if shap_values.ndim == 3:\n",
    "                # Shape: (n_samples, n_features, n_classes)\n",
    "                # Extract class 1 (Important) for first sample\n",
    "                print(f\"    [SHAP DEBUG] Extracting class 1 from shape {shap_values.shape}\")\n",
    "                attributions = shap_values[0, :, 1]  # [sample=0, all_features, class=1]\n",
    "            elif shap_values.ndim == 2:\n",
    "                # Shape: (n_samples, n_features)\n",
    "                # Already correct, just get first sample\n",
    "                attributions = shap_values[0, :]\n",
    "            else:\n",
    "                # Shape: (n_features,)\n",
    "                attributions = shap_values\n",
    "        \n",
    "        print(f\"    [SHAP DEBUG] Final attribution shape: {attributions.shape}\")\n",
    "        \n",
    "        # Final validation\n",
    "        expected_features = len(alert_tensor_scaled)\n",
    "        if len(attributions) != expected_features:\n",
    "            print(f\"    âš  WARNING: Attribution length {len(attributions)} != expected {expected_features}\")\n",
    "            print(f\"    Truncating/padding to match feature count\")\n",
    "            if len(attributions) > expected_features:\n",
    "                attributions = attributions[:expected_features]\n",
    "            else:\n",
    "                # Pad with zeros\n",
    "                attributions = np.pad(attributions, (0, expected_features - len(attributions)))\n",
    "        \n",
    "        return attributions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    âš  SHAP failed: {e}. Using gradient fallback.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return compute_gradient_attribution_fallback(alert_tensor_scaled)\n",
    "\n",
    "def compute_gradient_attribution_fallback(alert_features_scaled):\n",
    "    \"\"\"\n",
    "    Fallback: Simple gradient-based attribution\n",
    "    \"\"\"\n",
    "    alert_tensor = torch.tensor(alert_features_scaled, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    alert_tensor.requires_grad = True\n",
    "    \n",
    "    output = model(alert_tensor)\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "    \n",
    "    model.zero_grad()\n",
    "    output[0, pred_class].backward()\n",
    "    \n",
    "    gradients = alert_tensor.grad.squeeze().detach().cpu().numpy()\n",
    "    values = alert_tensor.squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    attributions = gradients * values\n",
    "    return attributions\n",
    "\n",
    "def is_missing_value(value, threshold=MISSING_VALUE_INDICATOR):\n",
    "    \"\"\"Check if a feature value represents missing/NA data\"\"\"\n",
    "    return abs(value - threshold) < 1e-6\n",
    "\n",
    "def generate_xai_explanation(model, alert_features, feature_names, top_k=5, scaler=None, shap_explainer=None):\n",
    "    \"\"\"\n",
    "    Generate XAI explanation using SHAP\n",
    "    \n",
    "    Args:\n",
    "        model: LSTM model\n",
    "        alert_features: numpy array of feature values (UNSCALED)\n",
    "        feature_names: list of feature names\n",
    "        top_k: number of top features\n",
    "        scaler: MinMaxScaler\n",
    "        shap_explainer: SHAP KernelExplainer instance\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with XAI results\n",
    "    \"\"\"\n",
    "    # Scale features\n",
    "    if scaler is not None:\n",
    "        alert_features_scaled = scaler.transform(alert_features.reshape(1, -1)).flatten()\n",
    "    else:\n",
    "        alert_features_scaled = alert_features\n",
    "    \n",
    "    # Get prediction\n",
    "    alert_tensor = torch.tensor(alert_features_scaled, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(alert_tensor)\n",
    "        probs = torch.softmax(output, dim=1)[0]\n",
    "        pred_class = output.argmax(dim=1).item()\n",
    "        confidence = probs[pred_class].item()\n",
    "    \n",
    "    print(f\"  [DEBUG] Prediction: {pred_class}, Confidence: {confidence:.4f}\")\n",
    "    \n",
    "    # Compute SHAP attributions\n",
    "    attributions = compute_shap_attribution(shap_explainer, alert_features_scaled, n_samples=50)\n",
    "    \n",
    "    print(f\"  [DEBUG] Attribution range: [{attributions.min():.4f}, {attributions.max():.4f}]\")\n",
    "    \n",
    "    # Combine features with attributions\n",
    "    feature_importance = []\n",
    "    for name, attr, value in zip(feature_names, attributions, alert_features):\n",
    "        feature_importance.append({\n",
    "            'feature': name,\n",
    "            'importance': float(attr),\n",
    "            'value': float(value),\n",
    "            'abs_importance': float(abs(attr)),\n",
    "            'is_missing': is_missing_value(value)\n",
    "        })\n",
    "    \n",
    "    # Sort by absolute importance\n",
    "    feature_importance.sort(key=lambda x: x['abs_importance'], reverse=True)\n",
    "    \n",
    "    # Filter out missing values\n",
    "    feature_importance_present = [f for f in feature_importance if not f['is_missing']]\n",
    "    \n",
    "    missing_count = len([f for f in feature_importance if f['is_missing']])\n",
    "    present_count = len(feature_importance_present)\n",
    "    \n",
    "    print(f\"  [DEBUG] Features: {present_count} present, {missing_count} missing\")\n",
    "    \n",
    "    top_features = feature_importance_present[:top_k] if len(feature_importance_present) >= top_k else feature_importance[:top_k]\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'Important' if pred_class == 1 else 'Irrelevant',\n",
    "        'confidence': confidence,\n",
    "        'pred_class': pred_class,\n",
    "        'top_features': top_features,\n",
    "        'all_features': feature_importance,\n",
    "        'num_missing_features': missing_count,\n",
    "        'num_present_features': present_count\n",
    "    }\n",
    "\n",
    "# ==================== CAUSAL COMPONENT ====================\n",
    "# [Keep all causal analysis functions from original code]\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CAUSAL COMPONENT: Root Cause Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def find_root_causes(graph, target_feature):\n",
    "    \"\"\"Find all root causes of target\"\"\"\n",
    "    if target_feature not in graph:\n",
    "        return []\n",
    "    ancestors = nx.ancestors(graph, target_feature)\n",
    "    return [node for node in ancestors if graph.in_degree(node) == 0]\n",
    "\n",
    "def find_causal_path(graph, source, target):\n",
    "    \"\"\"Find shortest causal path\"\"\"\n",
    "    if source not in graph or target not in graph:\n",
    "        return None\n",
    "    try:\n",
    "        return nx.shortest_path(graph, source, target)\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "def get_direct_causes(graph, feature):\n",
    "    \"\"\"Get direct causes (parents)\"\"\"\n",
    "    if feature not in graph:\n",
    "        return []\n",
    "    return list(graph.predecessors(feature))\n",
    "\n",
    "def analyze_causal_chain(graph, target_feature, alert_data, all_feature_names):\n",
    "    \"\"\"Analyze causal chains leading to target\"\"\"\n",
    "    if target_feature not in graph:\n",
    "        return {\n",
    "            'target': target_feature,\n",
    "            'in_graph': False,\n",
    "            'root_causes': [],\n",
    "            'causal_paths': [],\n",
    "            'direct_causes': [],\n",
    "            'reason': 'Feature not in causal graph'\n",
    "        }\n",
    "    \n",
    "    root_causes = find_root_causes(graph, target_feature)\n",
    "    \n",
    "    causal_paths = []\n",
    "    for root in root_causes:\n",
    "        path = find_causal_path(graph, root, target_feature)\n",
    "        if path:\n",
    "            path_with_values = [\n",
    "                {'feature': f, 'value': alert_data.get(f, 'N/A')}\n",
    "                for f in path\n",
    "            ]\n",
    "            causal_paths.append({\n",
    "                'root': root,\n",
    "                'path': path,\n",
    "                'path_with_values': path_with_values,\n",
    "                'length': len(path)\n",
    "            })\n",
    "    \n",
    "    direct_causes = get_direct_causes(graph, target_feature)\n",
    "    direct_causes_with_values = [\n",
    "        {'feature': c, 'value': alert_data.get(c, 'N/A')}\n",
    "        for c in direct_causes\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'target': target_feature,\n",
    "        'in_graph': True,\n",
    "        'root_causes': root_causes,\n",
    "        'causal_paths': causal_paths,\n",
    "        'direct_causes': direct_causes_with_values,\n",
    "        'num_paths': len(causal_paths)\n",
    "    }\n",
    "\n",
    "# ==================== HYBRID EXPLAINER ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYBRID EXPLAINER: Combining SHAP + Causal\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class HybridExplainer:\n",
    "    \"\"\"Combines SHAP and Causal Analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, model, causal_graph, feature_names, scaler=None, shap_explainer=None):\n",
    "        self.model = model\n",
    "        self.graph = causal_graph\n",
    "        self.feature_names = feature_names\n",
    "        self.scaler = scaler\n",
    "        self.shap_explainer = shap_explainer\n",
    "    \n",
    "    def explain(self, alert_data, alert_id=None):\n",
    "        \"\"\"Generate hybrid explanation\"\"\"\n",
    "        if isinstance(alert_data, dict):\n",
    "            alert_features = np.array([alert_data.get(f, 0) for f in self.feature_names])\n",
    "            alert_dict = alert_data\n",
    "        else:\n",
    "            alert_features = alert_data\n",
    "            alert_dict = {f: v for f, v in zip(self.feature_names, alert_features)}\n",
    "        \n",
    "        # Get XAI explanation using SHAP\n",
    "        xai_results = generate_xai_explanation(\n",
    "            self.model,\n",
    "            alert_features,\n",
    "            self.feature_names,\n",
    "            top_k=5,\n",
    "            scaler=self.scaler,\n",
    "            shap_explainer=self.shap_explainer\n",
    "        )\n",
    "        \n",
    "        # Analyze causal chains\n",
    "        causal_analyses = []\n",
    "        for feat_info in xai_results['top_features']:\n",
    "            feature_name = feat_info['feature']\n",
    "            if feature_name in self.graph:\n",
    "                causal_analysis = analyze_causal_chain(\n",
    "                    self.graph, feature_name, alert_dict, self.feature_names\n",
    "                )\n",
    "            else:\n",
    "                causal_analysis = {\n",
    "                    'target': feature_name,\n",
    "                    'in_graph': False,\n",
    "                    'root_causes': [],\n",
    "                    'causal_paths': [],\n",
    "                    'direct_causes': [],\n",
    "                    'reason': 'Feature not in SOC analyst-determined causal graph'\n",
    "                }\n",
    "            causal_analyses.append(causal_analysis)\n",
    "        \n",
    "        # Label causal analysis\n",
    "        label_causal = None\n",
    "        if 'Label' in self.graph:\n",
    "            label_causal = analyze_causal_chain(\n",
    "                self.graph, 'Label', alert_dict, self.feature_names\n",
    "            )\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = self._generate_recommendations(\n",
    "            xai_results, causal_analyses, alert_dict\n",
    "        )\n",
    "        \n",
    "        # Return HybridExplanation object\n",
    "        explanation = HybridExplanation(\n",
    "            alert_id=alert_id,\n",
    "            alert_data=alert_dict,\n",
    "            xai_results=xai_results,\n",
    "            causal_analyses=causal_analyses,\n",
    "            label_causal=label_causal,\n",
    "            recommendations=recommendations\n",
    "        )\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def _generate_recommendations(self, xai_results, causal_analyses, alert_data):\n",
    "        \"\"\"\n",
    "        Generate actionable recommendations based on XAI and causal analysis\n",
    "        \"\"\"\n",
    "        recommendations = {\n",
    "            'severity': 'MEDIUM',\n",
    "            'immediate_actions': [],\n",
    "            'investigation_steps': [],\n",
    "            'root_cause_mitigation': [],\n",
    "            'severity_reasoning': ''\n",
    "        }\n",
    "        \n",
    "        confidence = xai_results.get('confidence', 0.5)\n",
    "        prediction = xai_results.get('prediction', 'Unknown')\n",
    "        top_features = xai_results.get('top_features', [])\n",
    "        \n",
    "        if not top_features:\n",
    "            recommendations['immediate_actions'].append(\n",
    "                \"Alert requires manual review - feature analysis unavailable\"\n",
    "            )\n",
    "            return recommendations\n",
    "        \n",
    "        # Severity assessment for imbalanced dataset\n",
    "        if prediction == 'Important':\n",
    "            if confidence > 0.95:\n",
    "                recommendations['severity'] = 'HIGH'\n",
    "                recommendations['severity_reasoning'] = (\n",
    "                    f\"High-confidence Important alert ({confidence:.1%}). \"\n",
    "                    \"Given dataset imbalance (1.5% Important), high confidence indicates strong evidence.\"\n",
    "                )\n",
    "            elif confidence > 0.80:\n",
    "                recommendations['severity'] = 'MEDIUM'\n",
    "                recommendations['severity_reasoning'] = (\n",
    "                    f\"Medium-confidence Important alert ({confidence:.1%}). \"\n",
    "                    \"Requires further investigation to validate.\"\n",
    "                )\n",
    "            else:\n",
    "                recommendations['severity'] = 'LOW'\n",
    "                recommendations['severity_reasoning'] = (\n",
    "                    f\"Low-confidence Important alert ({confidence:.1%}). \"\n",
    "                    \"Model uncertain - likely borderline case requiring manual review.\"\n",
    "                )\n",
    "        else:\n",
    "            if confidence > 0.95:\n",
    "                recommendations['severity'] = 'MINIMAL'\n",
    "                recommendations['severity_reasoning'] = (\n",
    "                    f\"High-confidence Normal alert ({confidence:.1%}). \"\n",
    "                    \"Can likely be safely dismissed.\"\n",
    "                )\n",
    "            elif confidence > 0.80:\n",
    "                recommendations['severity'] = 'LOW'\n",
    "                recommendations['severity_reasoning'] = (\n",
    "                    f\"Medium-confidence Normal alert ({confidence:.1%}). \"\n",
    "                    \"Quick review recommended to confirm.\"\n",
    "                )\n",
    "            else:\n",
    "                recommendations['severity'] = 'MEDIUM'\n",
    "                recommendations['severity_reasoning'] = (\n",
    "                    f\"Low-confidence Normal alert ({confidence:.1%}). \"\n",
    "                    \"Uncertain classification - may be Important. Investigate carefully.\"\n",
    "                )\n",
    "        \n",
    "        # Analyze top features for specific actions\n",
    "        for feat in top_features:\n",
    "            feature = feat.get('feature', '')\n",
    "            value = feat.get('value', 0)\n",
    "            importance = feat.get('importance', 0)\n",
    "            \n",
    "            if feat.get('is_missing', False):\n",
    "                continue\n",
    "            \n",
    "            # STTL\n",
    "            if feature == 'sttl':\n",
    "                if value < 30:\n",
    "                    recommendations['immediate_actions'].append(\n",
    "                        f\"âš  CRITICAL: Abnormal TTL ({int(value)}) - Strong attack indicator\"\n",
    "                    )\n",
    "                    recommendations['investigation_steps'].append(\n",
    "                        \"Low TTL suggests packet spoofing or routing manipulation\"\n",
    "                    )\n",
    "            \n",
    "            # STATE\n",
    "            if feature == 'state':\n",
    "                state_map = {0: 'CON', 1: 'ECO', 2: 'FIN', 3: 'INT', \n",
    "                            4: 'PAR', 5: 'REQ', 6: 'RST', 7: 'URN', 8: 'no'}\n",
    "                state_name = state_map.get(int(value), 'Unknown')\n",
    "                \n",
    "                if state_name in ['REQ', 'INT']:\n",
    "                    recommendations['immediate_actions'].append(\n",
    "                        f\"âš  Suspicious connection state ({state_name})\"\n",
    "                    )\n",
    "            \n",
    "            # PROTO\n",
    "            if feature == 'proto':\n",
    "                proto_map = {6: 'TCP', 17: 'UDP', 1: 'ICMP'}\n",
    "                proto_name = proto_map.get(int(value), f'Protocol {int(value)}')\n",
    "                \n",
    "                if value == 1:\n",
    "                    recommendations['immediate_actions'].append(\n",
    "                        \"âš  ICMP protocol - Often used in reconnaissance or DDoS\"\n",
    "                    )\n",
    "            \n",
    "            # SCANNING INDICATOR\n",
    "            if feature == 'is_sm_ips_ports' and value == 1:\n",
    "                recommendations['immediate_actions'].append(\n",
    "                    \"âš  PORT SCAN DETECTED - Same source/dest IPs and ports\"\n",
    "                )\n",
    "        \n",
    "        # Causal-based recommendations\n",
    "        for causal in causal_analyses:\n",
    "            if causal.get('in_graph', False) and causal.get('root_causes'):\n",
    "                target = causal['target']\n",
    "                roots = causal['root_causes']\n",
    "                \n",
    "                recommendations['root_cause_mitigation'].append(\n",
    "                    f\"Root causes of {target}: {', '.join(roots)}\"\n",
    "                )\n",
    "        \n",
    "        # Default actions\n",
    "        if not recommendations['immediate_actions'] and prediction == 'Important':\n",
    "            recommendations['immediate_actions'].append(\n",
    "                \"Review alert details and correlate with other security events\"\n",
    "            )\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "class HybridExplanation:\n",
    "    \"\"\"Container for hybrid explanation results\"\"\"\n",
    "    \n",
    "    def __init__(self, alert_id, alert_data, xai_results, causal_analyses, \n",
    "                 label_causal, recommendations):\n",
    "        self.alert_id = alert_id\n",
    "        self.alert_data = alert_data\n",
    "        self.xai = xai_results\n",
    "        self.causal = causal_analyses\n",
    "        self.label_causal = label_causal\n",
    "        self.recommendations = recommendations\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dictionary with JSON-safe types\"\"\"\n",
    "        def make_json_safe(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                return {k: make_json_safe(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [make_json_safe(item) for item in obj]\n",
    "            elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, (np.bool_, bool)):\n",
    "                return bool(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "        data = {\n",
    "            'alert_id': self.alert_id,\n",
    "            'alert_data': self.alert_data,\n",
    "            'xai_analysis': self.xai,\n",
    "            'causal_analysis': self.causal,\n",
    "            'label_causal': self.label_causal,\n",
    "            'recommendations': self.recommendations\n",
    "        }\n",
    "        \n",
    "        return make_json_safe(data)\n",
    "    \n",
    "    def to_json(self, filepath=None):\n",
    "        \"\"\"Export to JSON\"\"\"\n",
    "        data = self.to_dict()\n",
    "        if filepath:\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "            return filepath\n",
    "        else:\n",
    "            return json.dumps(data, indent=2)\n",
    "    \n",
    "    def to_text(self):\n",
    "        \"\"\"Generate natural language explanation\"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"=\"*70)\n",
    "        lines.append(f\"HYBRID EXPLANATION - Alert #{self.alert_id or 'Unknown'}\")\n",
    "        lines.append(\"=\"*70)\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Classification\n",
    "        lines.append(\"ðŸŽ¯ CLASSIFICATION\")\n",
    "        lines.append(f\"  Prediction: {self.xai['prediction']}\")\n",
    "        lines.append(f\"  Confidence: {self.xai['confidence']:.1%}\")\n",
    "        lines.append(f\"  Severity: {self.recommendations['severity']}\")\n",
    "        if self.recommendations.get('severity_reasoning'):\n",
    "            lines.append(f\"  Reasoning: {self.recommendations['severity_reasoning']}\")\n",
    "        lines.append(f\"  Present Features: {self.xai['num_present_features']}/{len(FEATURE_NAMES)}\")\n",
    "        lines.append(f\"  Missing Features: {self.xai['num_missing_features']}/{len(FEATURE_NAMES)}\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # XAI Analysis\n",
    "        lines.append(\"ðŸ“Š XAI ANALYSIS (SHAP): What triggered this alert?\")\n",
    "        lines.append(\"  (Showing only features with values present in this alert)\")\n",
    "        lines.append(\"\")\n",
    "        for i, feat in enumerate(self.xai['top_features'], 1):\n",
    "            missing_marker = \" [MISSING]\" if feat.get('is_missing', False) else \"\"\n",
    "            lines.append(f\"  {i}. {feat['feature']}{missing_marker}\")\n",
    "            lines.append(f\"     Value: {feat['value']:.4f}\")\n",
    "            lines.append(f\"     Importance: {feat['importance']:.4f}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Causal Analysis\n",
    "        lines.append(\"ðŸ” CAUSAL ANALYSIS: Why/How did this happen?\")\n",
    "        lines.append(\"  (Limited to SOC analyst-identified features)\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        has_causal_info = False\n",
    "        for causal in self.causal:\n",
    "            if not causal.get('in_graph', False):\n",
    "                lines.append(f\"  Feature: {causal['target']}\")\n",
    "                lines.append(f\"  âš  {causal.get('reason', 'Not in causal graph')}\")\n",
    "                lines.append(\"\")\n",
    "                continue\n",
    "            \n",
    "            has_causal_info = True\n",
    "            lines.append(f\"  Feature: {causal['target']}\")\n",
    "            \n",
    "            if causal.get('root_causes'):\n",
    "                lines.append(f\"  Root Causes: {', '.join(causal['root_causes'])}\")\n",
    "            \n",
    "            if causal.get('causal_paths'):\n",
    "                lines.append(f\"  Causal Chains:\")\n",
    "                for path_info in causal['causal_paths'][:2]:\n",
    "                    path_str = ' â†’ '.join(path_info['path'])\n",
    "                    lines.append(f\"    â€¢ {path_str}\")\n",
    "            \n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Label causal\n",
    "        if self.label_causal and self.label_causal.get('in_graph'):\n",
    "            lines.append(\"  Direct Causes of Alert Classification:\")\n",
    "            for cause in self.label_causal.get('direct_causes', [])[:8]:\n",
    "                value = cause['value']\n",
    "                if isinstance(value, (int, float)) and value != -1.0:\n",
    "                    lines.append(f\"    â€¢ {cause['feature']} = {value:.4f}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Recommendations\n",
    "        lines.append(\"âœ… RECOMMENDED ACTIONS\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        if self.recommendations['immediate_actions']:\n",
    "            lines.append(\"  Immediate Actions:\")\n",
    "            for action in self.recommendations['immediate_actions']:\n",
    "                lines.append(f\"    â€¢ {action}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        if self.recommendations['investigation_steps']:\n",
    "            lines.append(\"  Investigation Steps:\")\n",
    "            for step in self.recommendations['investigation_steps']:\n",
    "                lines.append(f\"    â€¢ {step}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        if self.recommendations['root_cause_mitigation']:\n",
    "            lines.append(\"  Root Cause Mitigation:\")\n",
    "            for mitigation in self.recommendations['root_cause_mitigation']:\n",
    "                lines.append(f\"    â€¢ {mitigation}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        lines.append(\"=\"*70)\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def visualize(self, save_path=None):\n",
    "        \"\"\"Create visual explanation\"\"\"\n",
    "        fig = plt.figure(figsize=(16, 10))\n",
    "        gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        pred_emoji = \"âš ï¸\" if self.xai['prediction'] == 'Important' else \"âœ“\"\n",
    "        fig.suptitle(f'{pred_emoji} Hybrid Explanation (SHAP) - Alert #{self.alert_id or \"Unknown\"}', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. XAI Feature Importance\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        top_feats = self.xai['top_features']\n",
    "        features = [f['feature'] for f in top_feats]\n",
    "        importances = [f['importance'] for f in top_feats]\n",
    "        \n",
    "        colors = []\n",
    "        for feat in top_feats:\n",
    "            if feat.get('is_missing', False):\n",
    "                colors.append('gray')\n",
    "            elif feat['importance'] > 0:\n",
    "                colors.append('red')\n",
    "            else:\n",
    "                colors.append('blue')\n",
    "        \n",
    "        ax1.barh(features, importances, color=colors, alpha=0.7)\n",
    "        ax1.set_xlabel('SHAP Importance Score')\n",
    "        ax1.set_title('XAI Analysis: Top Feature Importance (SHAP)', fontweight='bold')\n",
    "        ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        ax1.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # 2. Prediction info\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        severity_color = {\n",
    "            'HIGH': 'ðŸ”´', 'MEDIUM': 'ðŸŸ¡', 'LOW': 'ðŸŸ¢', 'MINIMAL': 'âšª'\n",
    "        }.get(self.recommendations['severity'], 'âšª')\n",
    "        \n",
    "        pred_text = f\"\"\"\n",
    "PREDICTION\n",
    "{'='*30}\n",
    "Class: {self.xai['prediction']}\n",
    "Confidence: {self.xai['confidence']:.1%}\n",
    "Severity: {severity_color} {self.recommendations['severity']}\n",
    "\n",
    "Present Features: {self.xai['num_present_features']}/{len(FEATURE_NAMES)}\n",
    "Missing Features: {self.xai['num_missing_features']}/{len(FEATURE_NAMES)}\n",
    "\"\"\"\n",
    "        ax2.text(0.1, 0.5, pred_text, fontsize=10, family='monospace',\n",
    "                verticalalignment='center')\n",
    "        \n",
    "        # 3. Causal paths\n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        ax3.axis('off')\n",
    "        \n",
    "        causal_text = \"CAUSAL CHAINS\\n\" + \"=\"*30 + \"\\n\"\n",
    "        has_paths = False\n",
    "        for causal in self.causal[:3]:\n",
    "            if causal.get('in_graph') and causal.get('causal_paths'):\n",
    "                has_paths = True\n",
    "                for path_info in causal['causal_paths'][:1]:\n",
    "                    path_str = ' â†’ '.join(path_info['path'][:4])\n",
    "                    if len(path_info['path']) > 4:\n",
    "                        path_str += ' ...'\n",
    "                    causal_text += f\"â€¢ {path_str}\\n\"\n",
    "        \n",
    "        if not has_paths:\n",
    "            causal_text += \"\\nâš  Top features not in\\ncausal graph\\n\"\n",
    "        \n",
    "        ax3.text(0.1, 0.5, causal_text, fontsize=10, family='monospace',\n",
    "                verticalalignment='center')\n",
    "        \n",
    "        # 4. Recommendations\n",
    "        ax4 = fig.add_subplot(gs[2, :])\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        rec_text = \"RECOMMENDED ACTIONS\\n\" + \"=\"*50 + \"\\n\"\n",
    "        if self.recommendations['immediate_actions']:\n",
    "            rec_text += \"\\nImmediate:\\n\"\n",
    "            for action in self.recommendations['immediate_actions'][:3]:\n",
    "                shortened = action[:70] + \"...\" if len(action) > 70 else action\n",
    "                rec_text += f\"  â€¢ {shortened}\\n\"\n",
    "        \n",
    "        if self.recommendations['investigation_steps']:\n",
    "            rec_text += \"\\nInvestigation:\\n\"\n",
    "            for step in self.recommendations['investigation_steps'][:2]:\n",
    "                shortened = step[:70] + \"...\" if len(step) > 70 else step\n",
    "                rec_text += f\"  â€¢ {shortened}\\n\"\n",
    "        \n",
    "        ax4.text(0.05, 0.5, rec_text, fontsize=9, family='monospace',\n",
    "                verticalalignment='center')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"âœ“ Saved visualization to: {save_path}\")\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# ==================== DEMO ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING DEMO EXPLANATIONS WITH SHAP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "explainer = HybridExplainer(\n",
    "    model=model,\n",
    "    causal_graph=causal_graph,\n",
    "    feature_names=FEATURE_NAMES,\n",
    "    scaler=scaler,\n",
    "    shap_explainer=shap_explainer\n",
    ")\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\nGenerating explanations for sample alerts...\")\n",
    "    \n",
    "    # Check label distribution\n",
    "    if 'Label' in df.columns:\n",
    "        label_counts = df['Label'].value_counts()\n",
    "        print(f\"\\nDataset distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            pct = count / len(df) * 100\n",
    "            print(f\"  {label}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Select diverse samples\n",
    "    important_samples = []\n",
    "    irrelevant_samples = []\n",
    "    \n",
    "    if 'Label' in df.columns:\n",
    "        sample_label = df['Label'].iloc[0]\n",
    "        \n",
    "        if isinstance(sample_label, (int, np.integer)):\n",
    "            print(\"  Detected numeric labels (0=Normal, 1=Attack)\")\n",
    "            important_df = df[df['Label'] == 1]\n",
    "            irrelevant_df = df[df['Label'] == 0]\n",
    "        else:\n",
    "            print(\"  Detected string labels\")\n",
    "            important_df = df[df['Label'] == 'Important']\n",
    "            irrelevant_df = df[df['Label'] == 'Irrelevant']\n",
    "        \n",
    "        if len(important_df) > 0:\n",
    "            important_samples = important_df.sample(n=min(3, len(important_df)), random_state=42).index.tolist()\n",
    "        \n",
    "        if len(irrelevant_df) > 0:\n",
    "            irrelevant_samples = irrelevant_df.sample(n=min(2, len(irrelevant_df)), random_state=42).index.tolist()\n",
    "        \n",
    "        sample_indices = important_samples + irrelevant_samples\n",
    "        print(f\"\\nSelected {len(important_samples)} Important + {len(irrelevant_samples)} Normal alerts\")\n",
    "    else:\n",
    "        sample_indices = [0, 100, 500]\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        if idx >= len(df):\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        true_label = df.iloc[idx].get('Label', 'Unknown')\n",
    "        if isinstance(true_label, (int, np.integer)):\n",
    "            true_label_str = f\"{true_label}\"\n",
    "        else:\n",
    "            true_label_str = str(true_label)\n",
    "        print(f\"EXAMPLE ALERT #{idx} (True Label: {true_label_str})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Get alert data\n",
    "        alert_row = df.iloc[idx]\n",
    "        alert_features = alert_row[FEATURE_NAMES].values\n",
    "        \n",
    "        # Generate explanation\n",
    "        explanation = explainer.explain(alert_features, alert_id=idx)\n",
    "        \n",
    "        # Print text explanation\n",
    "        print(explanation.to_text())\n",
    "        \n",
    "        # Save JSON\n",
    "        json_path = f'hybrid_explanation_shap_{idx}.json'\n",
    "        explanation.to_json(json_path)\n",
    "        print(f\"\\nâœ“ Saved JSON to: {json_path}\")\n",
    "        \n",
    "        # Save visualization\n",
    "        viz_path = f'hybrid_explanation_shap_{idx}.png'\n",
    "        explanation.visualize(save_path=viz_path)\n",
    "        plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4 COMPLETE - USING SHAP\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - hybrid_explanation_shap_*.json (structured data)\")\n",
    "print(\"  - hybrid_explanation_shap_*.png (visualizations)\")\n",
    "print(\"\\nKey differences from DeepLIFT version:\")\n",
    "print(\"  âœ“ Uses SHAP KernelExplainer for model-agnostic explanations\")\n",
    "print(\"  âœ“ Background data: 100 samples from training set\")\n",
    "print(\"  âœ“ Same output format for Step 5 evaluation\")\n",
    "print(\"\\nNext: Step 5 - Evaluation (quantitative metrics + comparison)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
