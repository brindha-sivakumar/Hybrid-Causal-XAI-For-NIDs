{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce5814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 4: HYBRID EXPLANATION GENERATOR (FIXED)\n",
      "======================================================================\n",
      "\n",
      "Loading models and data...\n",
      "âœ“ Loaded LSTM model from ../step1_lstm_xai/best_lstm.pt\n",
      "âœ“ Loaded scaler from ../step1_lstm_xai/scaler.joblib\n",
      "âœ“ Loaded causal graph from ../step2_causal_discovery/causal_graph.gpickle\n",
      "  Nodes: 11, Edges: 20\n",
      "  Note: Causal graph limited to 10 SOC analyst-identified features\n",
      "âœ“ Loaded data from ../step2_causal_discovery/causal_discovery_data.csv: (10000, 11)\n",
      "  âš  Causal discovery data has only 11 columns\n",
      "  âš  LSTM needs 42 features\n",
      "  Loading original dataset instead...\n",
      "  âœ“ Loaded full dataset: (1395324, 47)\n",
      "  âœ“ Using full dataset with 43 features\n",
      "\n",
      "======================================================================\n",
      "XAI COMPONENT: Feature Importance (with Missing Value Filtering)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CAUSAL COMPONENT: Root Cause Analysis\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "HYBRID EXPLAINER: Combining XAI + Causal\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "GENERATING DEMO EXPLANATIONS\n",
      "======================================================================\n",
      "\n",
      "Generating explanations for diverse sample alerts...\n",
      "\n",
      "Dataset distribution:\n",
      "  0: 1,374,372 (98.5%)\n",
      "  1: 20,952 (1.5%)\n",
      "\n",
      "Available features: 42/42\n",
      "  Detected numeric labels (0=Irrelevant, 1=Important)\n",
      "\n",
      "Selected 3 Important + 2 Irrelevant alerts\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE ALERT #549227 (True Label: 1.0)\n",
      "======================================================================\n",
      "  [DEBUG] Scaled features range: [-28.9211, 1.0000]\n",
      "  [DEBUG] Prediction: 1, Confidence: 1.0000\n",
      "  [DEBUG] Attribution range: [-0.7923, 1.0047]\n",
      "  [DEBUG] Non-zero attributions: 37/42\n",
      "  [DEBUG] Features: 23 present, 19 missing (filtered)\n",
      "======================================================================\n",
      "HYBRID EXPLANATION - Alert #549227\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ¯ CLASSIFICATION\n",
      "  Prediction: Important\n",
      "  Confidence: 100.0%\n",
      "  Severity: HIGH\n",
      "  Reasoning: High-confidence Important alert (100.0%). Given dataset imbalance (1.5% Important), high confidence indicates strong evidence.\n",
      "  Present Features: 23/42\n",
      "  Missing Features: 19/42\n",
      "\n",
      "ðŸ“Š XAI ANALYSIS: What triggered this alert?\n",
      "  (Showing only features with values present in this alert)\n",
      "\n",
      "  1. AppProtoSimilarity\n",
      "     Value: 0.9989\n",
      "     Importance: 0.0694\n",
      "\n",
      "  2. IntPortSimilarity\n",
      "     Value: 0.9985\n",
      "     Importance: -0.0657\n",
      "\n",
      "  3. IntIPSimilarity\n",
      "     Value: 0.3963\n",
      "     Importance: 0.0603\n",
      "\n",
      "  4. SCAS\n",
      "     Value: 1.0000\n",
      "     Importance: 0.0465\n",
      "\n",
      "  5. HttpProtocolSimilarity\n",
      "     Value: 0.9988\n",
      "     Importance: 0.0417\n",
      "\n",
      "ðŸ” CAUSAL ANALYSIS: Why/How did this happen?\n",
      "  (Limited to 10 SOC analyst-identified features)\n",
      "\n",
      "  Feature: AppProtoSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: IntPortSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: IntIPSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: SCAS\n",
      "  Root Causes: SignatureMatchesPerDay\n",
      "  Causal Chains:\n",
      "    â€¢ SignatureMatchesPerDay â†’ SCAS\n",
      "\n",
      "  Feature: HttpProtocolSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Direct Causes of Alert Classification:\n",
      "    â€¢ SCAS = 1.0000\n",
      "    â€¢ SignatureMatchesPerDay = 0.0000\n",
      "    â€¢ Similarity = 0.3254\n",
      "    â€¢ Proto = 6.0000\n",
      "    â€¢ SignatureIDSimilarity = 0.0000\n",
      "    â€¢ ProtoSimilarity = 0.9999\n",
      "    â€¢ SignatureID = 2034159.0000\n",
      "    â€¢ IntPort = 80.0000\n",
      "\n",
      "âœ… RECOMMENDED ACTIONS\n",
      "\n",
      "  Immediate Actions:\n",
      "    â€¢ âš  OUTLIER DETECTED (SCAS=1) - Novel attack pattern not seen before\n",
      "\n",
      "  Investigation Steps:\n",
      "    â€¢ Compare with historical alerts - this is a unique pattern requiring manual analysis\n",
      "\n",
      "  Root Cause Mitigation:\n",
      "    â€¢ Root causes of SCAS: SignatureMatchesPerDay\n",
      "    â€¢ Mitigate high signature matches: Review and tune signature rules, implement rate limiting for repeat offenders\n",
      "\n",
      "======================================================================\n",
      "\n",
      "âœ“ Saved JSON to: hybrid_explanation_fixed_549227.json\n",
      "âœ“ Saved visualization to: hybrid_explanation_fixed_549227.png\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE ALERT #67703 (True Label: 1.0)\n",
      "======================================================================\n",
      "  [DEBUG] Scaled features range: [-28.9211, 1.0000]\n",
      "  [DEBUG] Prediction: 1, Confidence: 1.0000\n",
      "  [DEBUG] Attribution range: [-0.8141, 1.0089]\n",
      "  [DEBUG] Non-zero attributions: 38/42\n",
      "  [DEBUG] Features: 23 present, 19 missing (filtered)\n",
      "======================================================================\n",
      "HYBRID EXPLANATION - Alert #67703\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ¯ CLASSIFICATION\n",
      "  Prediction: Important\n",
      "  Confidence: 100.0%\n",
      "  Severity: HIGH\n",
      "  Reasoning: High-confidence Important alert (100.0%). Given dataset imbalance (1.5% Important), high confidence indicates strong evidence.\n",
      "  Present Features: 23/42\n",
      "  Missing Features: 19/42\n",
      "\n",
      "ðŸ“Š XAI ANALYSIS: What triggered this alert?\n",
      "  (Showing only features with values present in this alert)\n",
      "\n",
      "  1. AppProtoSimilarity\n",
      "     Value: 0.9093\n",
      "     Importance: 0.0628\n",
      "\n",
      "  2. SCAS\n",
      "     Value: 1.0000\n",
      "     Importance: 0.0516\n",
      "\n",
      "  3. IntPortSimilarity\n",
      "     Value: 0.5265\n",
      "     Importance: -0.0368\n",
      "\n",
      "  4. HttpProtocolSimilarity\n",
      "     Value: 0.8601\n",
      "     Importance: 0.0357\n",
      "\n",
      "  5. ProtoSimilarity\n",
      "     Value: 0.9619\n",
      "     Importance: 0.0179\n",
      "\n",
      "ðŸ” CAUSAL ANALYSIS: Why/How did this happen?\n",
      "  (Limited to 10 SOC analyst-identified features)\n",
      "\n",
      "  Feature: AppProtoSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: SCAS\n",
      "  Root Causes: SignatureMatchesPerDay\n",
      "  Causal Chains:\n",
      "    â€¢ SignatureMatchesPerDay â†’ SCAS\n",
      "\n",
      "  Feature: IntPortSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: HttpProtocolSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: ProtoSimilarity\n",
      "  Root Causes: SignatureMatchesPerDay\n",
      "  Causal Chains:\n",
      "    â€¢ SignatureMatchesPerDay â†’ ProtoSimilarity\n",
      "\n",
      "  Direct Causes of Alert Classification:\n",
      "    â€¢ SCAS = 1.0000\n",
      "    â€¢ SignatureMatchesPerDay = 20.6287\n",
      "    â€¢ Similarity = 0.2285\n",
      "    â€¢ Proto = 6.0000\n",
      "    â€¢ SignatureIDSimilarity = 0.0242\n",
      "    â€¢ ProtoSimilarity = 0.9619\n",
      "    â€¢ SignatureID = 2019182.0000\n",
      "    â€¢ IntPort = 80.0000\n",
      "\n",
      "âœ… RECOMMENDED ACTIONS\n",
      "\n",
      "  Immediate Actions:\n",
      "    â€¢ âš  OUTLIER DETECTED (SCAS=1) - Novel attack pattern not seen before\n",
      "\n",
      "  Investigation Steps:\n",
      "    â€¢ Compare with historical alerts - this is a unique pattern requiring manual analysis\n",
      "\n",
      "  Root Cause Mitigation:\n",
      "    â€¢ Root causes of SCAS: SignatureMatchesPerDay\n",
      "    â€¢ Mitigate high signature matches: Review and tune signature rules, implement rate limiting for repeat offenders\n",
      "    â€¢ Root causes of ProtoSimilarity: SignatureMatchesPerDay\n",
      "    â€¢ Mitigate high signature matches: Review and tune signature rules, implement rate limiting for repeat offenders\n",
      "\n",
      "======================================================================\n",
      "\n",
      "âœ“ Saved JSON to: hybrid_explanation_fixed_67703.json\n",
      "âœ“ Saved visualization to: hybrid_explanation_fixed_67703.png\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE ALERT #1086374 (True Label: 1.0)\n",
      "======================================================================\n",
      "  [DEBUG] Scaled features range: [-28.9211, 1.0000]\n",
      "  [DEBUG] Prediction: 1, Confidence: 1.0000\n",
      "  [DEBUG] Attribution range: [-0.8277, 0.9879]\n",
      "  [DEBUG] Non-zero attributions: 35/42\n",
      "  [DEBUG] Features: 24 present, 18 missing (filtered)\n",
      "======================================================================\n",
      "HYBRID EXPLANATION - Alert #1086374\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ¯ CLASSIFICATION\n",
      "  Prediction: Important\n",
      "  Confidence: 100.0%\n",
      "  Severity: HIGH\n",
      "  Reasoning: High-confidence Important alert (100.0%). Given dataset imbalance (1.5% Important), high confidence indicates strong evidence.\n",
      "  Present Features: 24/42\n",
      "  Missing Features: 18/42\n",
      "\n",
      "ðŸ“Š XAI ANALYSIS: What triggered this alert?\n",
      "  (Showing only features with values present in this alert)\n",
      "\n",
      "  1. AppProtoSimilarity\n",
      "     Value: 0.7163\n",
      "     Importance: 0.0521\n",
      "\n",
      "  2. SCAS\n",
      "     Value: 1.0000\n",
      "     Importance: 0.0510\n",
      "\n",
      "  3. HttpProtocolSimilarity\n",
      "     Value: 0.6874\n",
      "     Importance: 0.0307\n",
      "\n",
      "  4. Proto\n",
      "     Value: 6.0000\n",
      "     Importance: 0.0177\n",
      "\n",
      "  5. ProtoSimilarity\n",
      "     Value: 0.8737\n",
      "     Importance: 0.0163\n",
      "\n",
      "ðŸ” CAUSAL ANALYSIS: Why/How did this happen?\n",
      "  (Limited to 10 SOC analyst-identified features)\n",
      "\n",
      "  Feature: AppProtoSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: SCAS\n",
      "  Root Causes: SignatureMatchesPerDay\n",
      "  Causal Chains:\n",
      "    â€¢ SignatureMatchesPerDay â†’ SCAS\n",
      "\n",
      "  Feature: HttpProtocolSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: Proto\n",
      "  Root Causes: SignatureID, SignatureMatchesPerDay\n",
      "  Causal Chains:\n",
      "    â€¢ SignatureID â†’ Proto\n",
      "    â€¢ SignatureMatchesPerDay â†’ Proto\n",
      "\n",
      "  Feature: ProtoSimilarity\n",
      "  Root Causes: SignatureMatchesPerDay\n",
      "  Causal Chains:\n",
      "    â€¢ SignatureMatchesPerDay â†’ ProtoSimilarity\n",
      "\n",
      "  Direct Causes of Alert Classification:\n",
      "    â€¢ SCAS = 1.0000\n",
      "    â€¢ SignatureMatchesPerDay = 20.0686\n",
      "    â€¢ Similarity = 0.2196\n",
      "    â€¢ Proto = 6.0000\n",
      "    â€¢ SignatureIDSimilarity = 0.1009\n",
      "    â€¢ ProtoSimilarity = 0.8737\n",
      "    â€¢ SignatureID = 2011144.0000\n",
      "    â€¢ IntPort = 8008.0000\n",
      "\n",
      "âœ… RECOMMENDED ACTIONS\n",
      "\n",
      "  Immediate Actions:\n",
      "    â€¢ âš  OUTLIER DETECTED (SCAS=1) - Novel attack pattern not seen before\n",
      "\n",
      "  Investigation Steps:\n",
      "    â€¢ Compare with historical alerts - this is a unique pattern requiring manual analysis\n",
      "    â€¢ TCP traffic detected - Review connection state and payload\n",
      "\n",
      "  Root Cause Mitigation:\n",
      "    â€¢ Root causes of SCAS: SignatureMatchesPerDay\n",
      "    â€¢ Mitigate high signature matches: Review and tune signature rules, implement rate limiting for repeat offenders\n",
      "    â€¢ Root causes of Proto: SignatureID, SignatureMatchesPerDay\n",
      "    â€¢ Mitigate high signature matches: Review and tune signature rules, implement rate limiting for repeat offenders\n",
      "    â€¢ Root causes of ProtoSimilarity: SignatureMatchesPerDay\n",
      "    â€¢ Mitigate high signature matches: Review and tune signature rules, implement rate limiting for repeat offenders\n",
      "\n",
      "======================================================================\n",
      "\n",
      "âœ“ Saved JSON to: hybrid_explanation_fixed_1086374.json\n",
      "âœ“ Saved visualization to: hybrid_explanation_fixed_1086374.png\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE ALERT #1134888 (True Label: 0.0)\n",
      "======================================================================\n",
      "  [DEBUG] Scaled features range: [-28.9211, 1.0000]\n",
      "  [DEBUG] Prediction: 1, Confidence: 1.0000\n",
      "  [DEBUG] Attribution range: [-0.8552, 1.0482]\n",
      "  [DEBUG] Non-zero attributions: 36/42\n",
      "  [DEBUG] Features: 24 present, 18 missing (filtered)\n",
      "======================================================================\n",
      "HYBRID EXPLANATION - Alert #1134888\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ¯ CLASSIFICATION\n",
      "  Prediction: Important\n",
      "  Confidence: 100.0%\n",
      "  Severity: HIGH\n",
      "  Reasoning: High-confidence Important alert (100.0%). Given dataset imbalance (1.5% Important), high confidence indicates strong evidence.\n",
      "  Present Features: 24/42\n",
      "  Missing Features: 18/42\n",
      "\n",
      "ðŸ“Š XAI ANALYSIS: What triggered this alert?\n",
      "  (Showing only features with values present in this alert)\n",
      "\n",
      "  1. IntIPSimilarity\n",
      "     Value: 1.0000\n",
      "     Importance: 0.2022\n",
      "\n",
      "  2. SignatureIDSimilarity\n",
      "     Value: 1.0000\n",
      "     Importance: -0.1044\n",
      "\n",
      "  3. AppProtoSimilarity\n",
      "     Value: 1.0000\n",
      "     Importance: 0.0980\n",
      "\n",
      "  4. HttpUrlSimilarity\n",
      "     Value: 1.0000\n",
      "     Importance: 0.0831\n",
      "\n",
      "  5. HttpHostnameSimilarity\n",
      "     Value: 1.0000\n",
      "     Importance: 0.0742\n",
      "\n",
      "ðŸ” CAUSAL ANALYSIS: Why/How did this happen?\n",
      "  (Limited to 10 SOC analyst-identified features)\n",
      "\n",
      "  Feature: IntIPSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: SignatureIDSimilarity\n",
      "  Root Causes: SignatureMatchesPerDay\n",
      "  Causal Chains:\n",
      "    â€¢ SignatureMatchesPerDay â†’ SignatureIDSimilarity\n",
      "\n",
      "  Feature: AppProtoSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: HttpUrlSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: HttpHostnameSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Direct Causes of Alert Classification:\n",
      "    â€¢ SCAS = 0.0000\n",
      "    â€¢ SignatureMatchesPerDay = 410.8721\n",
      "    â€¢ Similarity = 0.6722\n",
      "    â€¢ Proto = 6.0000\n",
      "    â€¢ SignatureIDSimilarity = 1.0000\n",
      "    â€¢ ProtoSimilarity = 1.0000\n",
      "    â€¢ SignatureID = 2020899.0000\n",
      "    â€¢ IntPort = 80.0000\n",
      "\n",
      "âœ… RECOMMENDED ACTIONS\n",
      "\n",
      "  Immediate Actions:\n",
      "    â€¢ Review alert details and correlate with other security events in SIEM\n",
      "\n",
      "  Investigation Steps:\n",
      "    â€¢ Pattern closely matches known attacks (similarity=1.00) - Check threat intelligence for SignatureID=2020899\n",
      "\n",
      "  Root Cause Mitigation:\n",
      "    â€¢ Root causes of SignatureIDSimilarity: SignatureMatchesPerDay\n",
      "    â€¢ Mitigate high signature matches: Review and tune signature rules, implement rate limiting for repeat offenders\n",
      "\n",
      "======================================================================\n",
      "\n",
      "âœ“ Saved JSON to: hybrid_explanation_fixed_1134888.json\n",
      "âœ“ Saved visualization to: hybrid_explanation_fixed_1134888.png\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE ALERT #706915 (True Label: 0.0)\n",
      "======================================================================\n",
      "  [DEBUG] Scaled features range: [-28.9211, 1.0000]\n",
      "  [DEBUG] Prediction: 1, Confidence: 1.0000\n",
      "  [DEBUG] Attribution range: [-0.8718, 1.0317]\n",
      "  [DEBUG] Non-zero attributions: 41/42\n",
      "  [DEBUG] Features: 15 present, 27 missing (filtered)\n",
      "======================================================================\n",
      "HYBRID EXPLANATION - Alert #706915\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ¯ CLASSIFICATION\n",
      "  Prediction: Important\n",
      "  Confidence: 100.0%\n",
      "  Severity: HIGH\n",
      "  Reasoning: High-confidence Important alert (100.0%). Given dataset imbalance (1.5% Important), high confidence indicates strong evidence.\n",
      "  Present Features: 15/42\n",
      "  Missing Features: 27/42\n",
      "\n",
      "ðŸ“Š XAI ANALYSIS: What triggered this alert?\n",
      "  (Showing only features with values present in this alert)\n",
      "\n",
      "  1. ExtIPSimilarity\n",
      "     Value: 1.0000\n",
      "     Importance: -0.1334\n",
      "\n",
      "  2. IntIPSimilarity\n",
      "     Value: 1.0000\n",
      "     Importance: 0.1321\n",
      "\n",
      "  3. SignatureIDSimilarity\n",
      "     Value: 1.0000\n",
      "     Importance: -0.1285\n",
      "\n",
      "  4. IntPortSimilarity\n",
      "     Value: 1.0000\n",
      "     Importance: -0.0741\n",
      "\n",
      "  5. SignatureMatchesPerDay\n",
      "     Value: 138698.0419\n",
      "     Importance: -0.0605\n",
      "\n",
      "ðŸ” CAUSAL ANALYSIS: Why/How did this happen?\n",
      "  (Limited to 10 SOC analyst-identified features)\n",
      "\n",
      "  Feature: ExtIPSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: IntIPSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: SignatureIDSimilarity\n",
      "  Root Causes: SignatureMatchesPerDay\n",
      "  Causal Chains:\n",
      "    â€¢ SignatureMatchesPerDay â†’ SignatureIDSimilarity\n",
      "\n",
      "  Feature: IntPortSimilarity\n",
      "  âš  Feature not in SOC analyst-determined causal graph\n",
      "\n",
      "  Feature: SignatureMatchesPerDay\n",
      "\n",
      "  Direct Causes of Alert Classification:\n",
      "    â€¢ SCAS = 0.0000\n",
      "    â€¢ SignatureMatchesPerDay = 138698.0419\n",
      "    â€¢ Similarity = 1.0000\n",
      "    â€¢ Proto = 17.0000\n",
      "    â€¢ SignatureIDSimilarity = 1.0000\n",
      "    â€¢ ProtoSimilarity = 1.0000\n",
      "    â€¢ SignatureID = 2101411.0000\n",
      "    â€¢ IntPort = 161.0000\n",
      "\n",
      "âœ… RECOMMENDED ACTIONS\n",
      "\n",
      "  Immediate Actions:\n",
      "    â€¢ âš  EXTREMELY HIGH signature match frequency (138,698/day) - Potential coordinated campaign\n",
      "\n",
      "  Investigation Steps:\n",
      "    â€¢ Pattern closely matches known attacks (similarity=1.00) - Check threat intelligence for SignatureID=2101411\n",
      "    â€¢ Search for other hosts with SignatureMatchesPerDay > 50K in last 24h\n",
      "\n",
      "  Root Cause Mitigation:\n",
      "    â€¢ Review signature rules - 138,698 matches/day may indicate rule needs tuning\n",
      "    â€¢ Root causes of SignatureIDSimilarity: SignatureMatchesPerDay\n",
      "    â€¢ Mitigate high signature matches: Review and tune signature rules, implement rate limiting for repeat offenders\n",
      "\n",
      "======================================================================\n",
      "\n",
      "âœ“ Saved JSON to: hybrid_explanation_fixed_706915.json\n",
      "âœ“ Saved visualization to: hybrid_explanation_fixed_706915.png\n",
      "\n",
      "======================================================================\n",
      "STEP 4 COMPLETE! (FIXED VERSION)\n",
      "======================================================================\n",
      "\n",
      "Key Improvements:\n",
      "  âœ“ Missing value filtering (-1.0 indicators excluded from top features)\n",
      "  âœ“ Improved severity assessment for imbalanced dataset (1.5% vs 98.5%)\n",
      "  âœ“ Better handling of Irrelevant predictions\n",
      "  âœ“ Enhanced domain-specific recommendations\n",
      "  âœ“ Severity reasoning explanations\n",
      "\n",
      "Generated files:\n",
      "  - hybrid_explanation_fixed_*.json (structured data)\n",
      "  - hybrid_explanation_fixed_*.png (visualizations)\n",
      "\n",
      "Next: Step 5 - Evaluation (quantitative metrics + comparison)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Hybrid Explanation Generator - FIXED VERSION\n",
    "# Combines XAI (what) + Causal (why/how) for actionable NIDS alert explanations\n",
    "# FIXES:\n",
    "# 1. Feature filtering to exclude missing/NA values (-1.0)\n",
    "# 2. Improved severity assessment logic for imbalanced dataset\n",
    "# 3. Better handling of Irrelevant predictions\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 4: HYBRID EXPLANATION GENERATOR (FIXED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "LSTM_MODEL_PATH = '../step1_lstm_xai/best_lstm.pt'\n",
    "SCALER_PATH = '../step1_lstm_xai/scaler.joblib'\n",
    "CAUSAL_GRAPH_PATH = '../step2_causal_discovery/causal_graph.gpickle'\n",
    "DATA_PATH = '../UNSW_NB15_training-set.csv'\n",
    "\"\"\"\n",
    "# Feature names - MUST match the 42 features used in Step 1 LSTM training\n",
    "FEATURE_NAMES = [\n",
    "    'SignatureID', 'SignatureMatchesPerDay', 'AlertCount', 'Proto',\n",
    "    'ExtPort', 'IntPort', 'Similarity', 'SCAS', 'AppProtoSimilarity',\n",
    "    'DnsRrnameSimilarity', 'DnsRrtypeSimilarity', 'EmailFromSimilarity',\n",
    "    'EmailStatusSimilarity', 'EmailToSimilarity', 'ExtIPSimilarity',\n",
    "    'ExtPortSimilarity', 'HttpContentTypeSimilarity', 'HttpHostnameSimilarity',\n",
    "    'HttpMethodSimilarity', 'HttpProtocolSimilarity', 'HttpRequestBodySimilarity',\n",
    "    'HttpResponseBodySimilarity', 'HttpStatusSimilarity', 'HttpUrlSimilarity',\n",
    "    'HttpUserAgentSimilarity', 'IntIPSimilarity', 'IntPortSimilarity',\n",
    "    'ProtoSimilarity', 'SignatureIDSimilarity', 'SmtpHeloSimilarity',\n",
    "    'SmtpMailFromSimilarity', 'SmtpRcptToSimilarity', 'SshClientProtoSimilarity',\n",
    "    'SshClientSoftwareSimilarity', 'SshServerProtoSimilarity',\n",
    "    'SshServerSoftwareSimilarity', 'TlsFingerprintSimilarity',\n",
    "    'TlsIssuerDnSimilarity', 'TlsJa3hashSimilarity', 'TlsSniSimilarity',\n",
    "    'TlsSubjectSimilarity', 'TlsVersionSimilarity'\n",
    "]\n",
    "\n",
    "# Subset used in causal discovery (Step 2) - SOC analyst-determined important features\n",
    "CAUSAL_FEATURES = [\n",
    "    'SignatureMatchesPerDay', 'Similarity', 'SCAS', 'SignatureID',\n",
    "    'SignatureIDSimilarity', 'Proto', 'AlertCount', 'IntPort', \n",
    "    'ExtPort', 'ProtoSimilarity'\n",
    "]\n",
    "\"\"\"\n",
    "FEATURE_NAMES = [\n",
    "    'dur', 'proto', 'service', 'state', 'spkts', 'dpkts',\n",
    "    'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload',\n",
    "    'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit',\n",
    "    'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat',\n",
    "    'smean', 'dmean', 'trans_depth', 'response_body_len',\n",
    "    'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm',\n",
    "    'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login',\n",
    "    'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst',\n",
    "    'is_sm_ips_ports'\n",
    "]\n",
    "\n",
    "# Causal features (from Step 2)\n",
    "CAUSAL_FEATURES = [\n",
    "    'proto', 'sttl', 'state', 'dtcpb', 'is_sm_ips_ports',\n",
    "    'dttl', 'stcpb', 'service', 'dwin', 'swin'\n",
    "]\n",
    "# Missing value indicator\n",
    "MISSING_VALUE_INDICATOR = -1.0\n",
    "\n",
    "# ==================== LOAD MODELS AND DATA ====================\n",
    "print(\"\\nLoading models and data...\")\n",
    "\n",
    "# Load LSTM model architecture\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "# Load trained model\n",
    "device = torch.device('cpu')\n",
    "input_size = len(FEATURE_NAMES)\n",
    "model = LSTMClassifier(input_size=input_size).to(device)\n",
    "\n",
    "if Path(LSTM_MODEL_PATH).exists():\n",
    "    model.load_state_dict(torch.load(LSTM_MODEL_PATH, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"âœ“ Loaded LSTM model from {LSTM_MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"âš  Warning: {LSTM_MODEL_PATH} not found. Using untrained model.\")\n",
    "\n",
    "# Load scaler\n",
    "if Path(SCALER_PATH).exists():\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    print(f\"âœ“ Loaded scaler from {SCALER_PATH}\")\n",
    "else:\n",
    "    print(f\"âš  Warning: {SCALER_PATH} not found. Scaling may be incorrect.\")\n",
    "    scaler = None\n",
    "\n",
    "# Load causal graph\n",
    "if Path(CAUSAL_GRAPH_PATH).exists():\n",
    "    #causal_graph = nx.read_gpickle(CAUSAL_GRAPH_PATH)\n",
    "    causal_graph = pickle.load(open(CAUSAL_GRAPH_PATH, 'rb'))\n",
    "    print(f\"âœ“ Loaded causal graph from {CAUSAL_GRAPH_PATH}\")\n",
    "    print(f\"  Nodes: {causal_graph.number_of_nodes()}, Edges: {causal_graph.number_of_edges()}\")\n",
    "    print(f\"  Note: Causal graph limited to 10 SOC analyst-identified features\")\n",
    "else:\n",
    "    print(f\"âš  Warning: {CAUSAL_GRAPH_PATH} not found. Creating empty graph.\")\n",
    "    causal_graph = nx.DiGraph()\n",
    "\n",
    "# Load sample data for testing\n",
    "if Path(DATA_PATH).exists():\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"âœ“ Loaded data from {DATA_PATH}: {df.shape}\")\n",
    "    \n",
    "    if len(df.columns) < len(FEATURE_NAMES):\n",
    "        print(f\"  âš  Causal discovery data has only {len(df.columns)} columns\")\n",
    "        print(f\"  âš  LSTM needs {len(FEATURE_NAMES)} features\")\n",
    "        print(f\"  Loading original dataset instead...\")\n",
    "        \n",
    "        original_data_path = '../dataset-labeled-anon-ip.csv'\n",
    "        if Path(original_data_path).exists():\n",
    "            df_full = pd.read_csv(original_data_path)\n",
    "            print(f\"  âœ“ Loaded full dataset: {df_full.shape}\")\n",
    "            \n",
    "            # Drop non-feature columns\n",
    "            drop_cols = ['SignatureText', 'Timestamp', 'ExtIP', 'IntIP']\n",
    "            for col in drop_cols:\n",
    "                if col in df_full.columns:\n",
    "                    df_full = df_full.drop(columns=[col])\n",
    "            \n",
    "            df = df_full\n",
    "            print(f\"  âœ“ Using full dataset with {len(df.columns)} features\")\n",
    "        else:\n",
    "            print(f\"  âœ— Could not find {original_data_path}\")\n",
    "            df = None\n",
    "else:\n",
    "    print(f\"âš  Warning: {DATA_PATH} not found.\")\n",
    "    df = None\n",
    "\n",
    "#==================== ENCODE CATEGORICAL FEATURES ====================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ENCODING CATEGORICAL FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    # Drop non-feature columns first\n",
    "    drop_cols = ['id', 'attack_cat']\n",
    "    for col in drop_cols:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "    \n",
    "    # Create Label column if needed\n",
    "    if 'label' in df.columns and 'Label' not in df.columns:\n",
    "        df['Label'] = df['label']\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_cols = ['proto', 'service', 'state']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns and df[col].dtype == 'object':\n",
    "            print(f\"  Encoding {col}...\")\n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            print(f\"    âœ“ {col}: {df[col].nunique()} unique values\")\n",
    "    \n",
    "    # Verify all FEATURE_NAMES are numeric\n",
    "    print(\"\\n  Verifying features are numeric:\")\n",
    "    for feat in FEATURE_NAMES:\n",
    "        if feat in df.columns:\n",
    "            dtype = df[feat].dtype\n",
    "            is_numeric = dtype in ['int64', 'float64', 'int32', 'float32']\n",
    "            status = \"âœ“\" if is_numeric else \"âœ—\"\n",
    "            print(f\"    {status} {feat}: {dtype}\")\n",
    "            \n",
    "            if not is_numeric:\n",
    "                raise ValueError(f\"Feature {feat} is not numeric! dtype: {dtype}\")\n",
    "    \n",
    "    print(\"\\nâœ“ All features encoded and numeric\")\n",
    "\n",
    "\n",
    "# ==================== XAI COMPONENT ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"XAI COMPONENT: Feature Importance (with Missing Value Filtering)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def compute_deeplift_attribution(model, alert_tensor, baseline=None):\n",
    "    \"\"\"\n",
    "    Compute DeepLIFT attributions for a single alert\n",
    "    \n",
    "    Args:\n",
    "        model: LSTM model\n",
    "        alert_tensor: torch.Tensor of shape (1, 1, num_features) - MUST BE SCALED\n",
    "        baseline: Baseline for comparison (default: zeros)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of feature attributions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from captum.attr import DeepLift\n",
    "        \n",
    "        if baseline is None:\n",
    "            baseline = torch.zeros_like(alert_tensor)\n",
    "        \n",
    "        dl = DeepLift(model)\n",
    "        \n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(alert_tensor)\n",
    "            pred_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        # Compute attributions for predicted class\n",
    "        attributions = dl.attribute(alert_tensor, baselines=baseline, target=pred_class)\n",
    "        \n",
    "        return attributions.squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"âš  Captum not installed. Using gradient-based approximation.\")\n",
    "        return compute_gradient_attribution(model, alert_tensor)\n",
    "    except Exception as e:\n",
    "        print(f\"âš  DeepLIFT failed: {e}. Using gradient-based approximation.\")\n",
    "        return compute_gradient_attribution(model, alert_tensor)\n",
    "\n",
    "def compute_gradient_attribution(model, alert_tensor):\n",
    "    \"\"\"\n",
    "    Fallback: Simple gradient-based attribution\n",
    "    \"\"\"\n",
    "    alert_tensor.requires_grad = True\n",
    "    output = model(alert_tensor)\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "    \n",
    "    # Compute gradient\n",
    "    model.zero_grad()\n",
    "    output[0, pred_class].backward()\n",
    "    \n",
    "    gradients = alert_tensor.grad.squeeze().detach().cpu().numpy()\n",
    "    values = alert_tensor.squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    # Attribution = gradient * input\n",
    "    attributions = gradients * values\n",
    "    \n",
    "    return attributions\n",
    "\n",
    "def is_missing_value(value, threshold=MISSING_VALUE_INDICATOR):\n",
    "    \"\"\"\n",
    "    Check if a feature value represents missing/NA data\n",
    "    In this dataset, -1.0 indicates 'not applicable' or 'missing'\n",
    "    \"\"\"\n",
    "    return abs(value - threshold) < 1e-6\n",
    "\n",
    "def generate_xai_explanation(model, alert_features, feature_names, top_k=5, scaler=None):\n",
    "    \"\"\"\n",
    "    Generate XAI explanation for an alert\n",
    "    \n",
    "    FIXED: Now filters out missing values (-1.0) from top features\n",
    "    \n",
    "    Args:\n",
    "        model: LSTM model\n",
    "        alert_features: numpy array of feature values (UNSCALED from dataset)\n",
    "        feature_names: list of feature names\n",
    "        top_k: number of top features to return\n",
    "        scaler: MinMaxScaler used during training (REQUIRED for correct results!)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with XAI results\n",
    "    \"\"\"\n",
    "    # Scale the features before passing to model\n",
    "    if scaler is not None:\n",
    "        alert_features_scaled = scaler.transform(alert_features.reshape(1, -1)).flatten()\n",
    "        print(f\"  [DEBUG] Scaled features range: [{alert_features_scaled.min():.4f}, {alert_features_scaled.max():.4f}]\")\n",
    "    else:\n",
    "        print(\"  âš  WARNING: No scaler provided. Results may be incorrect!\")\n",
    "        alert_features_scaled = alert_features\n",
    "    \n",
    "    # Prepare input tensor (use SCALED features)\n",
    "    alert_tensor = torch.tensor(alert_features_scaled, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(alert_tensor)\n",
    "        probs = torch.softmax(output, dim=1)[0]\n",
    "        pred_class = output.argmax(dim=1).item()\n",
    "        confidence = probs[pred_class].item()\n",
    "    \n",
    "    print(f\"  [DEBUG] Prediction: {pred_class}, Confidence: {confidence:.4f}\")\n",
    "    \n",
    "    # Compute attributions\n",
    "    attributions = compute_deeplift_attribution(model, alert_tensor)\n",
    "    \n",
    "    print(f\"  [DEBUG] Attribution range: [{attributions.min():.4f}, {attributions.max():.4f}]\")\n",
    "    print(f\"  [DEBUG] Non-zero attributions: {np.count_nonzero(attributions)}/{len(attributions)}\")\n",
    "    \n",
    "    # Combine features with their attributions\n",
    "    feature_importance = []\n",
    "    for i, (name, attr, value) in enumerate(zip(feature_names, attributions, alert_features)):\n",
    "        feature_importance.append({\n",
    "            'feature': name,\n",
    "            'importance': float(attr),\n",
    "            'value': float(value),  # Use ORIGINAL unscaled value for interpretability\n",
    "            'abs_importance': float(abs(attr)),\n",
    "            'is_missing': is_missing_value(value)\n",
    "        })\n",
    "    \n",
    "    # Sort by absolute importance\n",
    "    feature_importance.sort(key=lambda x: x['abs_importance'], reverse=True)\n",
    "    \n",
    "    # FIX 1: Filter out missing values from top features\n",
    "    feature_importance_present = [\n",
    "        feat for feat in feature_importance \n",
    "        if not feat['is_missing']\n",
    "    ]\n",
    "    \n",
    "    missing_count = len([f for f in feature_importance if f['is_missing']])\n",
    "    present_count = len(feature_importance_present)\n",
    "    \n",
    "    print(f\"  [DEBUG] Features: {present_count} present, {missing_count} missing (filtered)\")\n",
    "    \n",
    "    # If not enough present features, fall back to all features\n",
    "    if len(feature_importance_present) < top_k:\n",
    "        print(f\"  âš  Warning: Only {len(feature_importance_present)} non-missing features available\")\n",
    "        top_features = feature_importance[:top_k]  # Use all including missing\n",
    "    else:\n",
    "        top_features = feature_importance_present[:top_k]  # Use only present features\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'Important' if pred_class == 1 else 'Irrelevant',\n",
    "        'confidence': confidence,\n",
    "        'pred_class': pred_class,\n",
    "        'top_features': top_features,\n",
    "        'all_features': feature_importance,\n",
    "        'num_missing_features': missing_count,\n",
    "        'num_present_features': present_count\n",
    "    }\n",
    "\n",
    "# ==================== CAUSAL COMPONENT ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CAUSAL COMPONENT: Root Cause Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def find_root_causes(graph, target_feature):\n",
    "    \"\"\"Find all root causes (ancestors with no incoming edges) of target\"\"\"\n",
    "    if target_feature not in graph:\n",
    "        return []\n",
    "    \n",
    "    ancestors = nx.ancestors(graph, target_feature)\n",
    "    root_causes = [node for node in ancestors if graph.in_degree(node) == 0]\n",
    "    \n",
    "    return root_causes\n",
    "\n",
    "def find_causal_path(graph, source, target):\n",
    "    \"\"\"Find shortest causal path from source to target\"\"\"\n",
    "    if source not in graph or target not in graph:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        path = nx.shortest_path(graph, source, target)\n",
    "        return path\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "def get_direct_causes(graph, feature):\n",
    "    \"\"\"Get direct causes (parents) of a feature\"\"\"\n",
    "    if feature not in graph:\n",
    "        return []\n",
    "    return list(graph.predecessors(feature))\n",
    "\n",
    "def analyze_causal_chain(graph, target_feature, alert_data, all_feature_names):\n",
    "    \"\"\"\n",
    "    Analyze causal chains leading to target feature\n",
    "    \n",
    "    Args:\n",
    "        graph: NetworkX causal graph (contains CAUSAL_FEATURES only)\n",
    "        target_feature: Feature to analyze (e.g., 'SCAS')\n",
    "        alert_data: Dictionary of ALL feature values (42 features)\n",
    "        all_feature_names: List of all feature names (42 features)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with causal analysis\n",
    "    \"\"\"\n",
    "    # Only analyze if feature is in the causal graph\n",
    "    if target_feature not in graph:\n",
    "        return {\n",
    "            'target': target_feature,\n",
    "            'in_graph': False,\n",
    "            'root_causes': [],\n",
    "            'causal_paths': [],\n",
    "            'direct_causes': [],\n",
    "            'reason': 'Feature not in SOC analyst-determined causal graph'\n",
    "        }\n",
    "    \n",
    "    # Find root causes\n",
    "    root_causes = find_root_causes(graph, target_feature)\n",
    "    \n",
    "    # Find causal paths from each root cause\n",
    "    causal_paths = []\n",
    "    for root in root_causes:\n",
    "        path = find_causal_path(graph, root, target_feature)\n",
    "        if path:\n",
    "            # Add feature values to path\n",
    "            path_with_values = []\n",
    "            for feature in path:\n",
    "                value = alert_data.get(feature, 'N/A')\n",
    "                path_with_values.append({\n",
    "                    'feature': feature,\n",
    "                    'value': value\n",
    "                })\n",
    "            \n",
    "            causal_paths.append({\n",
    "                'root': root,\n",
    "                'path': path,\n",
    "                'path_with_values': path_with_values,\n",
    "                'length': len(path)\n",
    "            })\n",
    "    \n",
    "    # Get direct causes\n",
    "    direct_causes = get_direct_causes(graph, target_feature)\n",
    "    direct_causes_with_values = []\n",
    "    for cause in direct_causes:\n",
    "        direct_causes_with_values.append({\n",
    "            'feature': cause,\n",
    "            'value': alert_data.get(cause, 'N/A')\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'target': target_feature,\n",
    "        'in_graph': True,\n",
    "        'root_causes': root_causes,\n",
    "        'causal_paths': causal_paths,\n",
    "        'direct_causes': direct_causes_with_values,\n",
    "        'num_paths': len(causal_paths)\n",
    "    }\n",
    "\n",
    "# ==================== HYBRID EXPLAINER ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYBRID EXPLAINER: Combining XAI + Causal\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class HybridExplainer:\n",
    "    \"\"\"\n",
    "    Combines XAI and Causal Analysis for comprehensive NIDS alert explanations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, causal_graph, feature_names, scaler=None):\n",
    "        self.model = model\n",
    "        self.graph = causal_graph\n",
    "        self.feature_names = feature_names\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def explain(self, alert_data, alert_id=None):\n",
    "        \"\"\"\n",
    "        Generate hybrid explanation for a single alert\n",
    "        \n",
    "        Args:\n",
    "            alert_data: numpy array (42 features) or dict of feature values\n",
    "            alert_id: Optional alert identifier\n",
    "        \n",
    "        Returns:\n",
    "            HybridExplanation object\n",
    "        \"\"\"\n",
    "        # Convert dict to array if needed\n",
    "        if isinstance(alert_data, dict):\n",
    "            alert_features = np.array([alert_data.get(f, 0) for f in self.feature_names])\n",
    "            alert_dict = alert_data\n",
    "        else:\n",
    "            alert_features = alert_data\n",
    "            alert_dict = {f: v for f, v in zip(self.feature_names, alert_features)}\n",
    "        \n",
    "        # Step 1: Get LSTM prediction and XAI explanation (uses all 42 features)\n",
    "        xai_results = generate_xai_explanation(\n",
    "            self.model, \n",
    "            alert_features, \n",
    "            self.feature_names,\n",
    "            top_k=5,\n",
    "            scaler=self.scaler\n",
    "        )\n",
    "        \n",
    "        # Step 2: Analyze causal chains for top XAI features\n",
    "        # Only analyze features that exist in causal graph\n",
    "        causal_analyses = []\n",
    "        for feat_info in xai_results['top_features']:\n",
    "            feature_name = feat_info['feature']\n",
    "            \n",
    "            # Only do causal analysis if feature is in causal graph\n",
    "            if feature_name in self.graph:\n",
    "                causal_analysis = analyze_causal_chain(\n",
    "                    self.graph,\n",
    "                    feature_name,\n",
    "                    alert_dict,\n",
    "                    self.feature_names\n",
    "                )\n",
    "                causal_analyses.append(causal_analysis)\n",
    "            else:\n",
    "                # Document why causal analysis is unavailable\n",
    "                causal_analyses.append({\n",
    "                    'target': feature_name,\n",
    "                    'in_graph': False,\n",
    "                    'root_causes': [],\n",
    "                    'causal_paths': [],\n",
    "                    'direct_causes': [],\n",
    "                    'reason': 'Feature not in SOC analyst-determined causal graph'\n",
    "                })\n",
    "        \n",
    "        # Step 3: Analyze causal chain to Label (outcome)\n",
    "        label_causal = None\n",
    "        if 'Label' in self.graph:\n",
    "            label_causal = analyze_causal_chain(\n",
    "                self.graph,\n",
    "                'Label',\n",
    "                alert_dict,\n",
    "                self.feature_names\n",
    "            )\n",
    "        \n",
    "        # Step 4: Generate recommendations\n",
    "        recommendations = self._generate_recommendations(\n",
    "            xai_results,\n",
    "            causal_analyses,\n",
    "            alert_dict\n",
    "        )\n",
    "        \n",
    "        # Create explanation object\n",
    "        explanation = HybridExplanation(\n",
    "            alert_id=alert_id,\n",
    "            alert_data=alert_dict,\n",
    "            xai_results=xai_results,\n",
    "            causal_analyses=causal_analyses,\n",
    "            label_causal=label_causal,\n",
    "            recommendations=recommendations\n",
    "        )\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def _generate_recommendations(self, xai_results, causal_analyses, alert_data):\n",
    "        \"\"\"\n",
    "        Generate actionable recommendations based on XAI and causal analysis\n",
    "        \n",
    "        FIX 2: Improved severity logic for imbalanced dataset\n",
    "        \n",
    "        recommendations = {\n",
    "            'severity': 'MEDIUM',\n",
    "            'immediate_actions': [],\n",
    "            'investigation_steps': [],\n",
    "            'root_cause_mitigation': [],\n",
    "            'severity_reasoning': ''\n",
    "        }\n",
    "        \"\"\"\n",
    "        recommendations = {\n",
    "            'severity': 'MEDIUM',\n",
    "            'immediate_actions': [],\n",
    "            'investigation_steps': [],\n",
    "            'root_cause_mitigation': [],\n",
    "            'severity_reasoning': ''\n",
    "        }\n",
    "        \n",
    "        # ... severity logic ...\n",
    "        \n",
    "        for feat in top_features:\n",
    "            feature = feat['feature']\n",
    "            value = feat['value']\n",
    "            # ========== STTL (STRONGEST CAUSAL INDICATOR) ==========\n",
    "            if feature == 'sttl':\n",
    "                if value < 30:\n",
    "                    recommendations['immediate_actions'].append(\n",
    "                        f\"âš  CRITICAL: Abnormal TTL ({int(value)}) - Strong attack indicator (Ï=0.739 with attacks)\"\n",
    "                    )\n",
    "                    recommendations['investigation_steps'].append(\n",
    "                        \"Low TTL suggests packet spoofing or routing manipulation - Check source IP reputation\"\n",
    "                    )\n",
    "                elif value > 250:\n",
    "                    recommendations['investigation_steps'].append(\n",
    "                        f\"High TTL ({int(value)}) - Possible legitimate traffic but verify source\"\n",
    "                    )\n",
    "            # ========== DTTL ==========\n",
    "            if feature == 'dttl':\n",
    "                if value < 30:\n",
    "                    recommendations['immediate_actions'].append(\n",
    "                        f\"âš  Abnormal destination TTL ({int(value)}) - Possible attack\"\n",
    "                    )\n",
    "            # ========== STATE (CAUSAL: proto â†’ state â†’ label) ==========\n",
    "            if feature == 'state':\n",
    "                state_map = {\n",
    "                    0: 'CON', 1: 'ECO', 2: 'FIN', 3: 'INT', \n",
    "                    4: 'PAR', 5: 'REQ', 6: 'RST', 7: 'URN', 8: 'no'\n",
    "                }\n",
    "                state_name = state_map.get(int(value), 'Unknown')\n",
    "                \n",
    "                if state_name in ['REQ', 'INT']:\n",
    "                    recommendations['immediate_actions'].append(\n",
    "                        f\"âš  Suspicious connection state ({state_name}) - Possible attack attempt\"\n",
    "                    )\n",
    "                elif state_name in ['RST', 'FIN']:\n",
    "                    recommendations['investigation_steps'].append(\n",
    "                        f\"Connection terminated ({state_name}) - May indicate failed attack or scan\"\n",
    "                    )\n",
    "            # ========== PROTO (MOST INFLUENTIAL - 4 outgoing edges) ==========\n",
    "            if feature == 'proto':\n",
    "                proto_map = {6: 'TCP', 17: 'UDP', 1: 'ICMP'}\n",
    "                proto_name = proto_map.get(int(value), f'Protocol {int(value)}')\n",
    "                \n",
    "                recommendations['investigation_steps'].append(\n",
    "                    f\"Protocol: {proto_name} - This determines connection state and scanning patterns\"\n",
    "                )\n",
    "                \n",
    "                if value == 1:  # ICMP\n",
    "                    recommendations['immediate_actions'].append(\n",
    "                        \"âš  ICMP protocol - Often used in reconnaissance or DDoS\"\n",
    "                    )\n",
    "                elif value == 17:  # UDP\n",
    "                    recommendations['investigation_steps'].append(\n",
    "                        \"UDP protocol - Check for amplification attacks\"\n",
    "                    )\n",
    "            # ========== WINDOW SIZES (negative correlation with attacks) ==========\n",
    "            if feature == 'swin' or feature == 'dwin':\n",
    "                window_name = \"Source\" if feature == 'swin' else \"Dest\"\n",
    "                if value == 0:\n",
    "                    recommendations['immediate_actions'].append(\n",
    "                        f\"âš  Zero {window_name} window - Abnormal TCP behavior indicating attack\"\n",
    "                    )\n",
    "                elif value < 100:\n",
    "                    recommendations['investigation_steps'].append(\n",
    "                        f\"Small {window_name} window ({int(value)}) - Possible malformed TCP\"\n",
    "                    )\n",
    "            \n",
    "            # ========== TCP SEQUENCES (negative correlation) ==========\n",
    "            if feature == 'dtcpb' or feature == 'stcpb':\n",
    "                seq_name = \"Destination\" if feature == 'dtcpb' else \"Source\"\n",
    "                if value == 0:\n",
    "                    recommendations['immediate_actions'].append(\n",
    "                        f\"âš  Zero {seq_name} TCP sequence - Malformed or crafted packet\"\n",
    "                    )\n",
    "            # ========== SCANNING INDICATOR (causal: proto â†’ is_sm_ips_ports) ==========\n",
    "            if feature == 'is_sm_ips_ports' and value == 1:\n",
    "                recommendations['immediate_actions'].append(\n",
    "                    \"âš  PORT SCAN DETECTED - Same source/dest IPs and ports\"\n",
    "                )\n",
    "                recommendations['investigation_steps'].append(\n",
    "                    \"Causal chain: Protocol type â†’ Scanning pattern - Check for other scanning signatures\"\n",
    "                )\n",
    "        \n",
    "        # ========== CAUSAL-BASED RECOMMENDATIONS ==========\n",
    "        for causal in causal_analyses:\n",
    "            if causal.get('in_graph', False) and causal.get('root_causes'):\n",
    "                target = causal['target']\n",
    "                roots = causal['root_causes']\n",
    "                \n",
    "                recommendations['root_cause_mitigation'].append(\n",
    "                    f\"Feature {target} is caused by: {', '.join(roots)}\"\n",
    "                )\n",
    "                \n",
    "                # Specific mitigations based on causal relationships\n",
    "                if 'proto' in roots:\n",
    "                    recommendations['root_cause_mitigation'].append(\n",
    "                        \"Protocol-level filtering: Implement protocol-specific firewall rules\"\n",
    "                    )\n",
    "                \n",
    "                if 'sttl' in roots or 'dttl' in roots:\n",
    "                    recommendations['root_cause_mitigation'].append(\n",
    "                        \"TTL-based filtering: Block packets with abnormal TTL values (< 30 or suspicious patterns)\"\n",
    "                    )\n",
    "        # FIX 2: Improved severity assessment for imbalanced dataset\n",
    "        confidence = xai_results['confidence']\n",
    "        prediction = xai_results['prediction']\n",
    "        \n",
    "        if prediction == 'Important':\n",
    "            # For Important alerts (1.5% of dataset)\n",
    "            # High confidence threshold because dataset is heavily skewed\n",
    "            if confidence > 0.95:\n",
    "                recommendations['severity'] = 'HIGH'\n",
    "                recommendations['severity_reasoning'] = (\n",
    "                    f\"High-confidence Important alert ({confidence:.1%}). \"\n",
    "                    \"Given dataset imbalance (1.5% Important), high confidence indicates strong evidence.\"\n",
    "                )\n",
    "            elif confidence > 0.80:\n",
    "                recommendations['severity'] = 'MEDIUM'\n",
    "                recommendations['severity_reasoning'] = (\n",
    "                    f\"Medium-confidence Important alert ({confidence:.1%}). \"\n",
    "                    \"Requires further investigation to validate.\"\n",
    "                )\n",
    "            else:\n",
    "                recommendations['severity'] = 'LOW'\n",
    "                recommendations['severity_reasoning'] = (\n",
    "                    f\"Low-confidence Important alert ({confidence:.1%}). \"\n",
    "                    \"Model uncertain - likely borderline case requiring manual review.\"\n",
    "                )\n",
    "        else:\n",
    "            # For Irrelevant alerts (98.5% of dataset)\n",
    "            if confidence > 0.95:\n",
    "                recommendations['severity'] = 'MINIMAL'\n",
    "                recommendations['severity_reasoning'] = (\n",
    "                    f\"High-confidence Irrelevant alert ({confidence:.1%}). \"\n",
    "                    \"Can likely be safely dismissed.\"\n",
    "                )\n",
    "            elif confidence > 0.80:\n",
    "                recommendations['severity'] = 'LOW'\n",
    "                recommendations['severity_reasoning'] = (\n",
    "                    f\"Medium-confidence Irrelevant alert ({confidence:.1%}). \"\n",
    "                    \"Quick review recommended to confirm.\"\n",
    "                )\n",
    "            else:\n",
    "                recommendations['severity'] = 'MEDIUM'\n",
    "                recommendations['severity_reasoning'] = (\n",
    "                    f\"Low-confidence Irrelevant alert ({confidence:.1%}). \"\n",
    "                    \"Uncertain classification - may be Important. Investigate carefully.\"\n",
    "                )\n",
    "        \n",
    "        # Analyze top features for specific actions\n",
    "        top_features = xai_results['top_features']\n",
    "        \n",
    "        for feat in top_features:\n",
    "            feature = feat['feature']\n",
    "            value = feat['value']\n",
    "            importance = feat['importance']\n",
    "            \n",
    "            # Skip if missing value\n",
    "            if feat.get('is_missing', False):\n",
    "                continue\n",
    "            \n",
    "            # SSH-related features\n",
    "            if feature == 'IntPort' and value == 22:\n",
    "                recommendations['immediate_actions'].append(\n",
    "                    \"SSH port (22) targeted - Enable SSH hardening (key-only auth, fail2ban)\"\n",
    "                )\n",
    "            \n",
    "            # DNS-related features\n",
    "            if feature == 'IntPort' and value == 53:\n",
    "                recommendations['immediate_actions'].append(\n",
    "                    \"DNS query detected - Check for DNS tunneling or exfiltration attempts\"\n",
    "                )\n",
    "            \n",
    "            # Outlier detection (SCAS = Similarity-based Contextual Anomaly Score)\n",
    "            if feature == 'SCAS' and value == 1:\n",
    "                recommendations['immediate_actions'].append(\n",
    "                    \"âš  OUTLIER DETECTED (SCAS=1) - Novel attack pattern not seen before\"\n",
    "                )\n",
    "                recommendations['investigation_steps'].append(\n",
    "                    \"Compare with historical alerts - this is a unique pattern requiring manual analysis\"\n",
    "                )\n",
    "                # Outliers should always be investigated\n",
    "                if recommendations['severity'] in ['MINIMAL', 'LOW']:\n",
    "                    recommendations['severity'] = 'MEDIUM'\n",
    "                    recommendations['severity_reasoning'] += \" Upgraded due to outlier detection.\"\n",
    "            \n",
    "            # High signature matching frequency\n",
    "            if feature == 'SignatureMatchesPerDay' and value > 50000:\n",
    "                recommendations['immediate_actions'].append(\n",
    "                    f\"âš  EXTREMELY HIGH signature match frequency ({int(value):,}/day) - Potential coordinated campaign\"\n",
    "                )\n",
    "                recommendations['investigation_steps'].append(\n",
    "                    f\"Search for other hosts with SignatureMatchesPerDay > 50K in last 24h\"\n",
    "                )\n",
    "                recommendations['root_cause_mitigation'].append(\n",
    "                    f\"Review signature rules - {int(value):,} matches/day may indicate rule needs tuning\"\n",
    "                )\n",
    "            elif feature == 'SignatureMatchesPerDay' and value > 10000:\n",
    "                recommendations['investigation_steps'].append(\n",
    "                    f\"High signature match frequency ({int(value):,}/day) - Monitor for escalation\"\n",
    "                )\n",
    "            \n",
    "            # High similarity to known attacks\n",
    "            if feature == 'SignatureIDSimilarity' and value > 0.95:\n",
    "                recommendations['investigation_steps'].append(\n",
    "                    f\"Pattern closely matches known attacks (similarity={value:.2f}) - \"\n",
    "                    f\"Check threat intelligence for SignatureID={int(alert_data.get('SignatureID', 0))}\"\n",
    "                )\n",
    "            \n",
    "            # Overall similarity score\n",
    "            if feature == 'Similarity' and value > 0.9:\n",
    "                recommendations['investigation_steps'].append(\n",
    "                    f\"High contextual similarity ({value:.2f}) - Alert matches common attack patterns\"\n",
    "                )\n",
    "            \n",
    "            # Alert volume/flooding\n",
    "            if feature == 'AlertCount' and value > 100:\n",
    "                recommendations['immediate_actions'].append(\n",
    "                    f\"âš  HIGH VOLUME ATTACK ({int(value)} alerts) - Consider rate limiting or blocking source\"\n",
    "                )\n",
    "            elif feature == 'AlertCount' and value > 10:\n",
    "                recommendations['investigation_steps'].append(\n",
    "                    f\"Multiple alerts ({int(value)}) from same source - Possible persistent attack\"\n",
    "                )\n",
    "            \n",
    "            # Protocol-specific recommendations\n",
    "            if feature == 'Proto':\n",
    "                if value == 6:  # TCP\n",
    "                    recommendations['investigation_steps'].append(\n",
    "                        \"TCP traffic detected - Review connection state and payload\"\n",
    "                    )\n",
    "                elif value == 17:  # UDP\n",
    "                    recommendations['investigation_steps'].append(\n",
    "                        \"UDP traffic detected - Check for amplification or flooding attacks\"\n",
    "                    )\n",
    "        \n",
    "        # Add causal-based recommendations\n",
    "        for causal in causal_analyses:\n",
    "            if causal.get('in_graph', False) and causal.get('root_causes'):\n",
    "                root_cause_str = ', '.join(causal['root_causes'])\n",
    "                recommendations['root_cause_mitigation'].append(\n",
    "                    f\"Root causes of {causal['target']}: {root_cause_str}\"\n",
    "                )\n",
    "                \n",
    "                # Add specific mitigation for known root causes\n",
    "                if 'SignatureMatchesPerDay' in causal['root_causes']:\n",
    "                    recommendations['root_cause_mitigation'].append(\n",
    "                        \"Mitigate high signature matches: Review and tune signature rules, \"\n",
    "                        \"implement rate limiting for repeat offenders\"\n",
    "                    )\n",
    "                \n",
    "                if 'Proto' in causal['root_causes']:\n",
    "                    recommendations['root_cause_mitigation'].append(\n",
    "                        \"Protocol-based attack: Consider protocol-specific firewall rules\"\n",
    "                    )\n",
    "        \n",
    "        # Default action if nothing specific found\n",
    "        if not recommendations['immediate_actions'] and prediction == 'Important':\n",
    "            recommendations['immediate_actions'].append(\n",
    "                \"Review alert details and correlate with other security events in SIEM\"\n",
    "            )\n",
    "        \n",
    "        if not recommendations['investigation_steps'] and prediction == 'Important':\n",
    "            recommendations['investigation_steps'].append(\n",
    "                f\"Investigate SignatureID {int(alert_data.get('SignatureID', 0))} in threat intelligence feeds\"\n",
    "            )\n",
    "        \n",
    "        # For Irrelevant alerts, add dismissal guidance\n",
    "        if prediction == 'Irrelevant' and confidence > 0.9:\n",
    "            recommendations['immediate_actions'].append(\n",
    "                \"Alert classified as Irrelevant with high confidence - Safe to dismiss after quick review\"\n",
    "            )\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "class HybridExplanation:\n",
    "    \"\"\"\n",
    "    Container for hybrid explanation results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alert_id, alert_data, xai_results, causal_analyses, \n",
    "                 label_causal, recommendations):\n",
    "        self.alert_id = alert_id\n",
    "        self.alert_data = alert_data\n",
    "        self.xai = xai_results\n",
    "        self.causal = causal_analyses\n",
    "        self.label_causal = label_causal\n",
    "        self.recommendations = recommendations\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dictionary with JSON-safe types\"\"\"\n",
    "        \n",
    "        def make_json_safe(obj):\n",
    "            \"\"\"Convert numpy types to native Python types\"\"\"\n",
    "            if isinstance(obj, dict):\n",
    "                return {k: make_json_safe(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [make_json_safe(item) for item in obj]\n",
    "            elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, (np.bool_, bool)):\n",
    "                return bool(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "        data = {\n",
    "            'alert_id': self.alert_id,\n",
    "            'alert_data': self.alert_data,\n",
    "            'xai_analysis': self.xai,\n",
    "            'causal_analysis': self.causal,\n",
    "            'label_causal': self.label_causal,\n",
    "            'recommendations': self.recommendations\n",
    "        }\n",
    "        \n",
    "        # Make all data JSON-safe\n",
    "        return make_json_safe(data)\n",
    "    \n",
    "    def to_json(self, filepath=None):\n",
    "        \"\"\"Export to JSON\"\"\"\n",
    "        data = self.to_dict()\n",
    "        if filepath:\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "            return filepath\n",
    "        else:\n",
    "            return json.dumps(data, indent=2)\n",
    "    \n",
    "    def to_text(self):\n",
    "        \"\"\"Generate natural language explanation\"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"=\"*70)\n",
    "        lines.append(f\"HYBRID EXPLANATION - Alert #{self.alert_id or 'Unknown'}\")\n",
    "        lines.append(\"=\"*70)\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Classification\n",
    "        lines.append(\"ðŸŽ¯ CLASSIFICATION\")\n",
    "        lines.append(f\"  Prediction: {self.xai['prediction']}\")\n",
    "        lines.append(f\"  Confidence: {self.xai['confidence']:.1%}\")\n",
    "        lines.append(f\"  Severity: {self.recommendations['severity']}\")\n",
    "        if self.recommendations.get('severity_reasoning'):\n",
    "            lines.append(f\"  Reasoning: {self.recommendations['severity_reasoning']}\")\n",
    "        lines.append(f\"  Present Features: {self.xai['num_present_features']}/42\")\n",
    "        lines.append(f\"  Missing Features: {self.xai['num_missing_features']}/42\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # XAI Analysis\n",
    "        lines.append(\"ðŸ“Š XAI ANALYSIS: What triggered this alert?\")\n",
    "        lines.append(\"  (Showing only features with values present in this alert)\")\n",
    "        lines.append(\"\")\n",
    "        for i, feat in enumerate(self.xai['top_features'], 1):\n",
    "            missing_marker = \" [MISSING]\" if feat.get('is_missing', False) else \"\"\n",
    "            lines.append(f\"  {i}. {feat['feature']}{missing_marker}\")\n",
    "            lines.append(f\"     Value: {feat['value']:.4f}\")\n",
    "            lines.append(f\"     Importance: {feat['importance']:.4f}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Causal Analysis\n",
    "        lines.append(\"ðŸ” CAUSAL ANALYSIS: Why/How did this happen?\")\n",
    "        lines.append(\"  (Limited to 10 SOC analyst-identified features)\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        has_causal_info = False\n",
    "        for causal in self.causal:\n",
    "            if not causal.get('in_graph', False):\n",
    "                lines.append(f\"  Feature: {causal['target']}\")\n",
    "                lines.append(f\"  âš  {causal.get('reason', 'Not in causal graph')}\")\n",
    "                lines.append(\"\")\n",
    "                continue\n",
    "            \n",
    "            has_causal_info = True\n",
    "            lines.append(f\"  Feature: {causal['target']}\")\n",
    "            \n",
    "            if causal.get('root_causes'):\n",
    "                lines.append(f\"  Root Causes: {', '.join(causal['root_causes'])}\")\n",
    "            \n",
    "            if causal.get('causal_paths'):\n",
    "                lines.append(f\"  Causal Chains:\")\n",
    "                for path_info in causal['causal_paths'][:2]:  # Show top 2 paths\n",
    "                    path_str = ' â†’ '.join(path_info['path'])\n",
    "                    lines.append(f\"    â€¢ {path_str}\")\n",
    "            \n",
    "            lines.append(\"\")\n",
    "        \n",
    "        if not has_causal_info:\n",
    "            lines.append(\"  âš  No top features found in causal graph\")\n",
    "            lines.append(\"  This indicates top XAI features are protocol-specific\")\n",
    "            lines.append(\"  (e.g., TLS, HTTP) and were not among SOC analyst-selected features\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Label causal analysis\n",
    "        if self.label_causal and self.label_causal.get('in_graph'):\n",
    "            lines.append(\"  Direct Causes of Alert Classification:\")\n",
    "            for cause in self.label_causal.get('direct_causes', []):\n",
    "                value = cause['value']\n",
    "                if isinstance(value, (int, float)) and value != -1.0:\n",
    "                    lines.append(f\"    â€¢ {cause['feature']} = {value:.4f}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Recommendations\n",
    "        lines.append(\"âœ… RECOMMENDED ACTIONS\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        if self.recommendations['immediate_actions']:\n",
    "            lines.append(\"  Immediate Actions:\")\n",
    "            for action in self.recommendations['immediate_actions']:\n",
    "                lines.append(f\"    â€¢ {action}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        if self.recommendations['investigation_steps']:\n",
    "            lines.append(\"  Investigation Steps:\")\n",
    "            for step in self.recommendations['investigation_steps']:\n",
    "                lines.append(f\"    â€¢ {step}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        if self.recommendations['root_cause_mitigation']:\n",
    "            lines.append(\"  Root Cause Mitigation:\")\n",
    "            for mitigation in self.recommendations['root_cause_mitigation']:\n",
    "                lines.append(f\"    â€¢ {mitigation}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        lines.append(\"=\"*70)\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def visualize(self, save_path=None):\n",
    "        \"\"\"Create visual explanation\"\"\"\n",
    "        fig = plt.figure(figsize=(16, 10))\n",
    "        gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Title\n",
    "        pred_emoji = \"âš ï¸\" if self.xai['prediction'] == 'Important' else \"âœ“\"\n",
    "        fig.suptitle(f'{pred_emoji} Hybrid Explanation - Alert #{self.alert_id or \"Unknown\"}', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. XAI Feature Importance (top subplot)\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        top_feats = self.xai['top_features']\n",
    "        features = [f['feature'] for f in top_feats]\n",
    "        importances = [f['importance'] for f in top_feats]\n",
    "        \n",
    "        # Color code: red for positive, blue for negative, gray for missing\n",
    "        colors = []\n",
    "        for feat in top_feats:\n",
    "            if feat.get('is_missing', False):\n",
    "                colors.append('gray')\n",
    "            elif feat['importance'] > 0:\n",
    "                colors.append('red')\n",
    "            else:\n",
    "                colors.append('blue')\n",
    "        \n",
    "        ax1.barh(features, importances, color=colors, alpha=0.7)\n",
    "        ax1.set_xlabel('Importance Score')\n",
    "        ax1.set_title('XAI Analysis: Top Feature Importance (Gray = Missing Values)', fontweight='bold')\n",
    "        ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        ax1.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # 2. Prediction info (middle left)\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        severity_color = {\n",
    "            'HIGH': 'ðŸ”´', 'MEDIUM': 'ðŸŸ¡', 'LOW': 'ðŸŸ¢', 'MINIMAL': 'âšª'\n",
    "        }.get(self.recommendations['severity'], 'âšª')\n",
    "        \n",
    "        pred_text = f\"\"\"\n",
    "PREDICTION\n",
    "{'='*30}\n",
    "Class: {self.xai['prediction']}\n",
    "Confidence: {self.xai['confidence']:.1%}\n",
    "Severity: {severity_color} {self.recommendations['severity']}\n",
    "\n",
    "Present Features: {self.xai['num_present_features']}/42\n",
    "Missing Features: {self.xai['num_missing_features']}/42\n",
    "\"\"\"\n",
    "        ax2.text(0.1, 0.5, pred_text, fontsize=10, family='monospace',\n",
    "                verticalalignment='center')\n",
    "        \n",
    "        # 3. Causal paths (middle right)\n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        ax3.axis('off')\n",
    "        \n",
    "        causal_text = \"CAUSAL CHAINS\\n\" + \"=\"*30 + \"\\n\"\n",
    "        has_paths = False\n",
    "        for causal in self.causal[:3]:\n",
    "            if causal.get('in_graph') and causal.get('causal_paths'):\n",
    "                has_paths = True\n",
    "                for path_info in causal['causal_paths'][:1]:\n",
    "                    path_str = ' â†’ '.join(path_info['path'][:4])\n",
    "                    if len(path_info['path']) > 4:\n",
    "                        path_str += ' ...'\n",
    "                    causal_text += f\"â€¢ {path_str}\\n\"\n",
    "        \n",
    "        if not has_paths:\n",
    "            causal_text += \"\\nâš  Top features not in\\nSOC analyst-selected\\ncausal graph\\n\"\n",
    "        \n",
    "        ax3.text(0.1, 0.5, causal_text, fontsize=10, family='monospace',\n",
    "                verticalalignment='center')\n",
    "        \n",
    "        # 4. Recommendations (bottom)\n",
    "        ax4 = fig.add_subplot(gs[2, :])\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        rec_text = \"RECOMMENDED ACTIONS\\n\" + \"=\"*50 + \"\\n\"\n",
    "        if self.recommendations['immediate_actions']:\n",
    "            rec_text += \"\\nImmediate:\\n\"\n",
    "            for action in self.recommendations['immediate_actions'][:3]:\n",
    "                shortened = action[:70] + \"...\" if len(action) > 70 else action\n",
    "                rec_text += f\"  â€¢ {shortened}\\n\"\n",
    "        \n",
    "        if self.recommendations['investigation_steps']:\n",
    "            rec_text += \"\\nInvestigation:\\n\"\n",
    "            for step in self.recommendations['investigation_steps'][:2]:\n",
    "                shortened = step[:70] + \"...\" if len(step) > 70 else step\n",
    "                rec_text += f\"  â€¢ {shortened}\\n\"\n",
    "        \n",
    "        ax4.text(0.05, 0.5, rec_text, fontsize=9, family='monospace',\n",
    "                verticalalignment='center')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"âœ“ Saved visualization to: {save_path}\")\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# ==================== DEMO EXAMPLES ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING DEMO EXPLANATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create explainer instance\n",
    "explainer = HybridExplainer(\n",
    "    model=model,\n",
    "    causal_graph=causal_graph,\n",
    "    feature_names=FEATURE_NAMES,\n",
    "    scaler=scaler\n",
    ")\n",
    "\n",
    "# Example 1: Use data from file if available\n",
    "if df is not None and len(df) > 0:\n",
    "    print(\"\\nGenerating explanations for diverse sample alerts...\")\n",
    "    \n",
    "    # ==================== ENCODE CATEGORICAL FEATURES (ADD THIS!) ====================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ENCODING CATEGORICAL FEATURES FOR STEP 4\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    import joblib\n",
    "    \n",
    "    # Drop non-feature columns\n",
    "    drop_cols = ['id', 'attack_cat']\n",
    "    for col in drop_cols:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "            print(f\"  Dropped: {col}\")\n",
    "    \n",
    "    # Handle label column\n",
    "    if 'attack_cat' in df.columns and 'label' not in df.columns:\n",
    "        df['label'] = (df['attack_cat'] != 'Normal').astype(int)\n",
    "    if 'label' in df.columns and 'Label' not in df.columns:\n",
    "        df['Label'] = df['label']\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_cols = ['proto', 'service', 'state']\n",
    "    \n",
    "    print(\"\\nEncoding categorical columns...\")\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            # Check if already numeric\n",
    "            if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "                print(f\"  âœ“ {col} already numeric\")\n",
    "                continue\n",
    "            \n",
    "            # Encode string values\n",
    "            print(f\"  Encoding {col}...\")\n",
    "            print(f\"    Original dtype: {df[col].dtype}\")\n",
    "            print(f\"    Sample values: {df[col].head(3).tolist()}\")\n",
    "            \n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            \n",
    "            print(f\"    âœ“ Encoded to: {df[col].dtype}\")\n",
    "            print(f\"    New values: {df[col].unique()[:5].tolist()}\")\n",
    "    \n",
    "    # Verify all features are numeric\n",
    "    print(\"\\n  Verifying all features are numeric:\")\n",
    "    all_numeric = True\n",
    "    for feat in FEATURE_NAMES:\n",
    "        if feat in df.columns:\n",
    "            dtype = df[feat].dtype\n",
    "            is_numeric = dtype in ['int64', 'float64', 'int32', 'float32']\n",
    "            if not is_numeric:\n",
    "                print(f\"    âœ— {feat}: {dtype} - NOT NUMERIC!\")\n",
    "                print(f\"      Sample: {df[feat].head(3).tolist()}\")\n",
    "                all_numeric = False\n",
    "    \n",
    "    if not all_numeric:\n",
    "        raise ValueError(\"Some features are not numeric! Cannot proceed.\")\n",
    "    else:\n",
    "        print(\"    âœ“ All features are numeric\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    # ==================== END ENCODING SECTION ====================\n",
    "\n",
    "\n",
    "    # Check label distribution\n",
    "    if 'Label' in df.columns:\n",
    "        label_counts = df['Label'].value_counts()\n",
    "        print(f\"\\nDataset distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            pct = count / len(df) * 100\n",
    "            print(f\"  {label}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Ensure we have all required features\n",
    "    available_features = [f for f in FEATURE_NAMES if f in df.columns]\n",
    "    print(f\"\\nAvailable features: {len(available_features)}/{len(FEATURE_NAMES)}\")\n",
    "    \n",
    "    if len(available_features) < len(FEATURE_NAMES):\n",
    "        print(\"âš  Warning: Not all features available. Creating synthetic examples...\")\n",
    "        df = None\n",
    "    else:\n",
    "        # Select diverse samples: Important alerts AND Irrelevant alerts\n",
    "        important_samples = []\n",
    "        irrelevant_samples = []\n",
    "        \n",
    "        if 'Label' in df.columns:\n",
    "            # Check if labels are numeric (0, 1) or string ('Important', 'Irrelevant')\n",
    "            sample_label = df['Label'].iloc[0]\n",
    "            \n",
    "            if isinstance(sample_label, (int, np.integer)):\n",
    "                # Numeric labels: 1 = Important, 0 = Irrelevant\n",
    "                print(\"  Detected numeric labels (0=Irrelevant, 1=Important)\")\n",
    "                important_df = df[df['Label'] == 1]\n",
    "                irrelevant_df = df[df['Label'] == 0]\n",
    "            else:\n",
    "                # String labels\n",
    "                print(\"  Detected string labels\")\n",
    "                important_df = df[df['Label'] == 'Important']\n",
    "                irrelevant_df = df[df['Label'] == 'Irrelevant']\n",
    "            \n",
    "            # Get some Important alerts\n",
    "            if len(important_df) > 0:\n",
    "                important_samples = important_df.sample(n=min(3, len(important_df)), random_state=42).index.tolist()\n",
    "            \n",
    "            # Get some Irrelevant alerts\n",
    "            if len(irrelevant_df) > 0:\n",
    "                irrelevant_samples = irrelevant_df.sample(n=min(2, len(irrelevant_df)), random_state=42).index.tolist()\n",
    "            \n",
    "            sample_indices = important_samples + irrelevant_samples\n",
    "            print(f\"\\nSelected {len(important_samples)} Important + {len(irrelevant_samples)} Irrelevant alerts\")\n",
    "        else:\n",
    "            sample_indices = [0, 100, 500, 1000, 5000]\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            if idx >= len(df):\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            true_label = df.iloc[idx].get('Label', 'Unknown')\n",
    "            # Convert numeric label to string for display\n",
    "            if isinstance(true_label, (int, np.integer)):\n",
    "                true_label_str = 'Important' if true_label == 1 else 'Irrelevant'\n",
    "            else:\n",
    "                true_label_str = str(true_label)\n",
    "            print(f\"EXAMPLE ALERT #{idx} (True Label: {true_label_str})\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            # Get alert data (all 42 features)\n",
    "            alert_row = df.iloc[idx]\n",
    "            alert_features = alert_row[FEATURE_NAMES].values\n",
    "            \n",
    "            # Generate explanation\n",
    "            explanation = explainer.explain(alert_features, alert_id=idx)\n",
    "            \n",
    "            # Print text explanation\n",
    "            print(explanation.to_text())\n",
    "            \n",
    "            # Save JSON\n",
    "            json_path = f'hybrid_explanation_fixed_{idx}.json'\n",
    "            explanation.to_json(json_path)\n",
    "            print(f\"\\nâœ“ Saved JSON to: {json_path}\")\n",
    "            \n",
    "            # Save visualization\n",
    "            viz_path = f'hybrid_explanation_fixed_{idx}.png'\n",
    "            explanation.visualize(save_path=viz_path)\n",
    "            plt.close()\n",
    "\n",
    "if df is None:\n",
    "    print(\"\\nâš  No data available for demo. Creating synthetic examples...\")\n",
    "    \n",
    "    # Create synthetic alerts for diverse scenarios\n",
    "    \n",
    "    # Scenario 1: SSH brute force (Important)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SYNTHETIC EXAMPLE 1: SSH Brute Force Attack\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    ssh_alert = {f: 0.0 for f in FEATURE_NAMES}\n",
    "    ssh_alert.update({\n",
    "        'SignatureMatchesPerDay': 50000,\n",
    "        'Similarity': 0.95,\n",
    "        'SCAS': 1.0,  # Outlier\n",
    "        'SignatureID': 2001219,\n",
    "        'SignatureIDSimilarity': 0.98,\n",
    "        'Proto': 6,  # TCP\n",
    "        'AlertCount': 250,\n",
    "        'IntPort': 22,  # SSH\n",
    "        'ExtPort': 54321,\n",
    "        'ProtoSimilarity': 1.0,\n",
    "        'IntPortSimilarity': 0.95,\n",
    "        # Mark TLS features as N/A\n",
    "        'TlsFingerprintSimilarity': -1.0,\n",
    "        'TlsIssuerDnSimilarity': -1.0,\n",
    "        'TlsSubjectSimilarity': -1.0,\n",
    "        'TlsVersionSimilarity': -1.0,\n",
    "        'TlsSniSimilarity': -1.0,\n",
    "        'TlsJa3hashSimilarity': -1.0\n",
    "    })\n",
    "    \n",
    "    explanation = explainer.explain(ssh_alert, alert_id=\"SSH-001\")\n",
    "    print(explanation.to_text())\n",
    "    explanation.to_json('hybrid_explanation_fixed_ssh_attack.json')\n",
    "    explanation.visualize(save_path='hybrid_explanation_fixed_ssh_attack.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Scenario 2: Benign DNS query (Irrelevant)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SYNTHETIC EXAMPLE 2: Benign DNS Query\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    dns_alert = {f: 0.0 for f in FEATURE_NAMES}\n",
    "    dns_alert.update({\n",
    "        'SignatureMatchesPerDay': 50,\n",
    "        'Similarity': 0.3,  # Low similarity\n",
    "        'SCAS': 0.0,  # Not an outlier\n",
    "        'SignatureID': 2027757,\n",
    "        'SignatureIDSimilarity': 0.4,\n",
    "        'Proto': 17,  # UDP\n",
    "        'AlertCount': 1,\n",
    "        'IntPort': 53,  # DNS\n",
    "        'ExtPort': 49152,\n",
    "        'ProtoSimilarity': 0.5,\n",
    "        'DnsRrnameSimilarity': 0.8,\n",
    "        'DnsRrtypeSimilarity': 0.9,\n",
    "        # Mark irrelevant features as N/A\n",
    "        'TlsFingerprintSimilarity': -1.0,\n",
    "        'TlsIssuerDnSimilarity': -1.0,\n",
    "        'TlsSubjectSimilarity': -1.0,\n",
    "        'HttpContentTypeSimilarity': -1.0,\n",
    "        'HttpStatusSimilarity': -1.0\n",
    "    })\n",
    "    \n",
    "    explanation = explainer.explain(dns_alert, alert_id=\"DNS-001\")\n",
    "    print(explanation.to_text())\n",
    "    explanation.to_json('hybrid_explanation_fixed_dns_benign.json')\n",
    "    explanation.visualize(save_path='hybrid_explanation_fixed_dns_benign.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Scenario 3: HTTP attack with high confidence (Important)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SYNTHETIC EXAMPLE 3: HTTP Attack\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    http_alert = {f: 0.0 for f in FEATURE_NAMES}\n",
    "    http_alert.update({\n",
    "        'SignatureMatchesPerDay': 15000,\n",
    "        'Similarity': 0.88,\n",
    "        'SCAS': 0.0,\n",
    "        'SignatureID': 2842116,\n",
    "        'SignatureIDSimilarity': 0.92,\n",
    "        'Proto': 6,  # TCP\n",
    "        'AlertCount': 50,\n",
    "        'IntPort': 80,  # HTTP\n",
    "        'ExtPort': 45678,\n",
    "        'ProtoSimilarity': 0.95,\n",
    "        'HttpContentTypeSimilarity': 0.85,\n",
    "        'HttpStatusSimilarity': 0.90,\n",
    "        'HttpMethodSimilarity': 0.88,\n",
    "        'HttpUrlSimilarity': 0.92,\n",
    "        'AppProtoSimilarity': 0.95,\n",
    "        # Mark TLS as N/A (HTTP not HTTPS)\n",
    "        'TlsFingerprintSimilarity': -1.0,\n",
    "        'TlsIssuerDnSimilarity': -1.0,\n",
    "        'TlsSubjectSimilarity': -1.0\n",
    "    })\n",
    "    \n",
    "    explanation = explainer.explain(http_alert, alert_id=\"HTTP-001\")\n",
    "    print(explanation.to_text())\n",
    "    explanation.to_json('hybrid_explanation_fixed_http_attack.json')\n",
    "    explanation.visualize(save_path='hybrid_explanation_fixed_http_attack.png')\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4 COMPLETE! (FIXED VERSION)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Improvements:\")\n",
    "print(\"  âœ“ Missing value filtering (-1.0 indicators excluded from top features)\")\n",
    "print(\"  âœ“ Improved severity assessment for imbalanced dataset (1.5% vs 98.5%)\")\n",
    "print(\"  âœ“ Better handling of Irrelevant predictions\")\n",
    "print(\"  âœ“ Enhanced domain-specific recommendations\")\n",
    "print(\"  âœ“ Severity reasoning explanations\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - hybrid_explanation_fixed_*.json (structured data)\")\n",
    "print(\"  - hybrid_explanation_fixed_*.png (visualizations)\")\n",
    "print(\"\\nNext: Step 5 - Evaluation (quantitative metrics + comparison)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
