{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82856df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, math, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cpu')  # M1 Mac - use CPU\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# ==================== DATA LOADING ====================\n",
    "#DATA_PATH = '../dataset-labeled-anon-ip.csv'\n",
    "DATA_PATH = '../UNSW_NB15_training-set.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f'Loaded {DATA_PATH}, shape = {df.shape}')\n",
    "\n",
    "# ==================== PREPROCESSING ====================\n",
    "#label_col = 'Label'\n",
    "label_col = 'label'\n",
    "drop_cols = ['SignatureText', 'Timestamp', 'ExtIP', 'IntIP']\n",
    "\n",
    "for c in drop_cols:\n",
    "    if c in df.columns:\n",
    "        df = df.drop(columns=[c])\n",
    "\n",
    "df = df.replace({-1: np.nan})\n",
    "df = df.fillna(0)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c != label_col]\n",
    "print(f'Feature count: {len(feature_cols)}')\n",
    "\n",
    "# Encode categorical features\n",
    "for c in feature_cols:\n",
    "    if df[c].dtype == 'object':\n",
    "        df[c] = LabelEncoder().fit_transform(df[c].astype(str))\n",
    "\n",
    "# Balance dataset\n",
    "n_per_class = 10000\n",
    "sampled = []\n",
    "for _, g in df.groupby(label_col):\n",
    "    sampled.append(g.sample(n=min(n_per_class, len(g)), random_state=seed))\n",
    "df_bal = pd.concat(sampled).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "print(f'Balanced df shape: {df_bal.shape}')\n",
    "\n",
    "X = df_bal[feature_cols].values\n",
    "y = df_bal[label_col].values\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.20, stratify=y, random_state=seed\n",
    ")\n",
    "print(f'Train/Test shapes: {X_train.shape}, {X_test.shape}')\n",
    "\n",
    "# Store number of features globally\n",
    "NUM_FEATURES = X_train.shape[1]\n",
    "print(f'Number of features: {NUM_FEATURES}')\n",
    "\n",
    "# ==================== DATASET CLASS ====================\n",
    "class AlertDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].unsqueeze(0), self.y[idx]\n",
    "\n",
    "# ==================== LSTM MODEL ====================\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "# Load or create model\n",
    "input_size = X_train.shape[1]\n",
    "model = LSTMClassifier(input_size=input_size, hidden_size=128, num_layers=2, dropout=0.2)\n",
    "\n",
    "if Path('best_lstm.pt').exists():\n",
    "    model.load_state_dict(torch.load('best_lstm.pt', map_location=device))\n",
    "    print('Loaded existing model from best_lstm.pt')\n",
    "else:\n",
    "    print('Warning: best_lstm.pt not found. Train the model first.')\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ==================== XAI SETUP ====================\n",
    "# Ground truth features from SOC analysts\n",
    "soc_features = ['SignatureMatchesPerDay', 'Similarity', 'SCAS',\n",
    "                'SignatureID', 'SignatureIDSimilarity']\n",
    "gt_indices = [feature_cols.index(f) for f in soc_features if f in feature_cols]\n",
    "print(f'Ground truth indices: {gt_indices}')\n",
    "\n",
    "# Sample size for evaluation (start smaller for M1)\n",
    "NUM_SAMPLES = 500  # Increase to 2000 if memory allows\n",
    "test_subset_X = X_test[:NUM_SAMPLES]\n",
    "test_subset_y = y_test[:NUM_SAMPLES]\n",
    "print(f'Evaluating on {NUM_SAMPLES} test samples')\n",
    "\n",
    "# ==================== EVALUATION METRICS ====================\n",
    "def normalize_attributions(attributions):\n",
    "    \"\"\"Ensure attributions have correct shape [num_samples, num_features]\"\"\"\n",
    "    original_shape = attributions.shape\n",
    "    \n",
    "    # Handle different input shapes\n",
    "    if attributions.ndim == 1:\n",
    "        # If 1D, reshape to [1, num_features]\n",
    "        attributions = attributions.reshape(1, -1)\n",
    "    elif attributions.ndim == 3:\n",
    "        # Check if this looks like [samples, features, classes]\n",
    "        if attributions.shape[2] <= 5 and attributions.shape[1] == NUM_FEATURES:\n",
    "            print(f\"Warning: Detected class-wise attributions {original_shape}, extracting last class\")\n",
    "            attributions = attributions[:, :, -1]\n",
    "        # Check if middle dimension is 1 (sequence dimension)\n",
    "        elif attributions.shape[1] == 1:\n",
    "            attributions = attributions.squeeze(1)\n",
    "        # If shape is [num_samples, seq_len, num_features] where seq_len > 1\n",
    "        else:\n",
    "            print(f\"Warning: 3D attribution with shape {original_shape}, taking last timestep\")\n",
    "            attributions = attributions[:, -1, :]\n",
    "    elif attributions.ndim > 3:\n",
    "        # Reshape to 2D by flattening extra dimensions\n",
    "        print(f\"Warning: {attributions.ndim}D attribution, reshaping to 2D\")\n",
    "        attributions = attributions.reshape(attributions.shape[0], -1)\n",
    "    \n",
    "    # Now attributions should be 2D: [num_samples, features]\n",
    "    if attributions.ndim != 2:\n",
    "        raise ValueError(f\"Could not normalize attributions to 2D, got shape {attributions.shape}\")\n",
    "    \n",
    "    # Ensure we have exactly NUM_FEATURES per sample\n",
    "    if attributions.shape[1] != NUM_FEATURES:\n",
    "        print(f\"Warning: Attribution shape {attributions.shape} doesn't match NUM_FEATURES {NUM_FEATURES}\")\n",
    "        print(f\"  Original shape was: {original_shape}\")\n",
    "        # Truncate or pad to match\n",
    "        if attributions.shape[1] > NUM_FEATURES:\n",
    "            print(f\"  Truncating from {attributions.shape[1]} to {NUM_FEATURES} features\")\n",
    "            attributions = attributions[:, :NUM_FEATURES]\n",
    "        else:\n",
    "            print(f\"  Padding from {attributions.shape[1]} to {NUM_FEATURES} features\")\n",
    "            padding = np.zeros((attributions.shape[0], NUM_FEATURES - attributions.shape[1]))\n",
    "            attributions = np.concatenate([attributions, padding], axis=1)\n",
    "    \n",
    "    return attributions\n",
    "\n",
    "def compute_faithfulness_correlation(model, X, attributions):\n",
    "    \"\"\"High Faithfulness Correlation metric\"\"\"\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    \n",
    "    # Normalize attributions\n",
    "    attributions = normalize_attributions(attributions)\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        x_input = torch.tensor(X[i:i+1], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        attr = attributions[i]  # Already 1D after normalization\n",
    "\n",
    "        # Get original prediction\n",
    "        with torch.no_grad():\n",
    "            orig_pred = torch.softmax(model(x_input), dim=1)[0, 1].item()\n",
    "\n",
    "        # Randomly sample feature subsets\n",
    "        n_subsets = 50\n",
    "        sum_attrs = []\n",
    "        pred_diffs = []\n",
    "\n",
    "        for _ in range(n_subsets):\n",
    "            # Random subset - Ensure indices are within the valid range\n",
    "            subset_size = np.random.randint(1, min(NUM_FEATURES, len(attr)) + 1)\n",
    "            subset_indices = np.random.choice(min(NUM_FEATURES, len(attr)), subset_size, replace=False)\n",
    "\n",
    "            # Sum of attributions in subset\n",
    "            sum_attr = np.sum(np.abs(attr[subset_indices]))\n",
    "            sum_attrs.append(sum_attr)\n",
    "\n",
    "            # Create masked input (set subset to zero/baseline)\n",
    "            x_masked = X[i].copy()\n",
    "            x_masked[subset_indices] = 0\n",
    "            x_masked_t = torch.tensor(x_masked[np.newaxis, np.newaxis, :],\n",
    "                                     dtype=torch.float32).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                masked_pred = torch.softmax(model(x_masked_t), dim=1)[0, 1].item()\n",
    "\n",
    "            pred_diffs.append(orig_pred - masked_pred)\n",
    "\n",
    "        # Compute correlation\n",
    "        if len(sum_attrs) > 1 and np.std(sum_attrs) > 0 and np.std(pred_diffs) > 0:\n",
    "            corr, _ = spearmanr(sum_attrs, pred_diffs)\n",
    "            if not np.isnan(corr):\n",
    "                scores.append(corr)\n",
    "\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "def compute_monotonicity(model, X, attributions):\n",
    "    \"\"\"Monotonicity metric\"\"\"\n",
    "    model.eval()\n",
    "    monotonic_count = 0\n",
    "    total_pairs = 0\n",
    "    \n",
    "    # Normalize attributions\n",
    "    attributions = normalize_attributions(attributions)\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]\n",
    "        attr = attributions[i]  # Already 1D after normalization\n",
    "\n",
    "        # Ensure we don't exceed bounds\n",
    "        attr_len = min(len(attr), NUM_FEATURES, len(x))\n",
    "        \n",
    "        # Get top 5 features\n",
    "        top_k = min(5, attr_len)\n",
    "        top_indices = np.argsort(-np.abs(attr[:attr_len]))[:top_k]\n",
    "\n",
    "        for idx in top_indices:\n",
    "            # Ensure idx is within bounds\n",
    "            if idx >= len(x):\n",
    "                continue\n",
    "                \n",
    "            # Original prediction\n",
    "            x_orig = torch.tensor(x[np.newaxis, np.newaxis, :],\n",
    "                                 dtype=torch.float32).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred_orig = torch.softmax(model(x_orig), dim=1)[0, 1].item()\n",
    "\n",
    "            # Perturbed prediction (set feature to baseline)\n",
    "            x_pert = x.copy()\n",
    "            x_pert[idx] = 0\n",
    "            x_pert_t = torch.tensor(x_pert[np.newaxis, np.newaxis, :],\n",
    "                                   dtype=torch.float32).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred_pert = torch.softmax(model(x_pert_t), dim=1)[0, 1].item()\n",
    "\n",
    "            # Check monotonicity\n",
    "            if attr[idx] > 0:\n",
    "                if pred_orig >= pred_pert:\n",
    "                    monotonic_count += 1\n",
    "            else:\n",
    "                if pred_orig <= pred_pert:\n",
    "                    monotonic_count += 1\n",
    "\n",
    "            total_pairs += 1\n",
    "\n",
    "    return monotonic_count / total_pairs if total_pairs > 0 else 0\n",
    "\n",
    "def compute_max_sensitivity(X, attributions, radius=0.1):\n",
    "    \"\"\"Max Sensitivity metric\"\"\"\n",
    "    sensitivities = []\n",
    "    \n",
    "    # Normalize attributions\n",
    "    attributions = normalize_attributions(attributions)\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]\n",
    "        attr = attributions[i]\n",
    "\n",
    "        # Find nearest neighbors within radius\n",
    "        max_diff = 0\n",
    "        for j in range(len(X)):\n",
    "            if i == j:\n",
    "                continue\n",
    "\n",
    "            dist = euclidean(x, X[j])\n",
    "            if dist <= radius:\n",
    "                attr_j = attributions[j]\n",
    "                diff = euclidean(attr, attr_j)\n",
    "                max_diff = max(max_diff, diff)\n",
    "\n",
    "        if max_diff > 0:\n",
    "            sensitivities.append(max_diff)\n",
    "\n",
    "    return np.mean(sensitivities) if sensitivities else 0, np.std(sensitivities) if sensitivities else 0\n",
    "\n",
    "def compute_low_complexity(attributions):\n",
    "    \"\"\"Low Complexity (entropy) metric\"\"\"\n",
    "    complexities = []\n",
    "    \n",
    "    # Normalize attributions\n",
    "    attributions = normalize_attributions(attributions)\n",
    "\n",
    "    for attr in attributions:\n",
    "        magnitudes = np.abs(attr)\n",
    "\n",
    "        if magnitudes.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        p = magnitudes / magnitudes.sum()\n",
    "        entropy = -np.sum([pi * np.log(pi + 1e-12) for pi in p if pi > 0])\n",
    "        complexities.append(entropy)\n",
    "\n",
    "    return np.mean(complexities) if complexities else 0, np.std(complexities) if complexities else 0\n",
    "\n",
    "def compute_RMA(attributions, ground_truth_indices):\n",
    "    \"\"\"Relevance Mass Accuracy\"\"\"\n",
    "    rma_scores = []\n",
    "    \n",
    "    # Normalize attributions\n",
    "    attributions = normalize_attributions(attributions)\n",
    "    \n",
    "    # Filter ground truth indices to be within bounds\n",
    "    valid_gt_indices = [idx for idx in ground_truth_indices if idx < NUM_FEATURES]\n",
    "\n",
    "    for attr in attributions:\n",
    "        attr_len = min(len(attr), NUM_FEATURES)\n",
    "        valid_indices = [idx for idx in valid_gt_indices if idx < attr_len]\n",
    "        \n",
    "        if not valid_indices:\n",
    "            continue\n",
    "            \n",
    "        numerator = np.sum(np.abs(attr[valid_indices]))\n",
    "        denominator = np.sum(np.abs(attr[:attr_len]))\n",
    "\n",
    "        if denominator > 0:\n",
    "            rma_scores.append(numerator / denominator)\n",
    "\n",
    "    return np.mean(rma_scores) if rma_scores else 0, np.std(rma_scores) if rma_scores else 0\n",
    "\n",
    "def compute_RRA(attributions, ground_truth_indices):\n",
    "    \"\"\"Relevance Rank Accuracy\"\"\"\n",
    "    rra_scores = []\n",
    "    \n",
    "    # Normalize attributions\n",
    "    attributions = normalize_attributions(attributions)\n",
    "    \n",
    "    # Filter ground truth indices to be within bounds\n",
    "    valid_gt_indices = [idx for idx in ground_truth_indices if idx < NUM_FEATURES]\n",
    "    K = len(valid_gt_indices)\n",
    "    \n",
    "    if K == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    for attr in attributions:\n",
    "        attr_len = min(len(attr), NUM_FEATURES)\n",
    "        top_k = np.argsort(-np.abs(attr[:attr_len]))[:K]\n",
    "        overlap = len(set(top_k).intersection(set(valid_gt_indices)))\n",
    "        rra_scores.append(overlap / K)\n",
    "\n",
    "    return np.mean(rra_scores) if rra_scores else 0, np.std(rra_scores) if rra_scores else 0\n",
    "\n",
    "# ==================== EXPLAINER FUNCTIONS ====================\n",
    "print(\"\\nComputing explanations for all explainers...\")\n",
    "\n",
    "# 1. LIME\n",
    "print(\"\\n1. Computing LIME explanations...\")\n",
    "try:\n",
    "    from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "    lime_explainer = LimeTabularExplainer(\n",
    "        X_train,\n",
    "        feature_names=feature_cols,\n",
    "        class_names=['irrelevant', 'important'],\n",
    "        discretize_continuous=True\n",
    "    )\n",
    "\n",
    "    def predict_proba_numpy(x):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            arr = torch.tensor(x, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            out = model(arr)\n",
    "            probs = torch.softmax(out, dim=1).cpu().numpy()\n",
    "        return probs\n",
    "\n",
    "    lime_attrs = []\n",
    "    for i in tqdm(range(NUM_SAMPLES), desc=\"LIME\"):\n",
    "        exp = lime_explainer.explain_instance(\n",
    "            test_subset_X[i],\n",
    "            predict_proba_numpy,\n",
    "            num_features=len(feature_cols)\n",
    "        )\n",
    "\n",
    "        # Extract feature importance as array\n",
    "        feature_importance = np.zeros(len(feature_cols))\n",
    "        for idx, weight in exp.as_list():\n",
    "            # Parse feature index from lime string\n",
    "            feat_name = idx.split()[0]\n",
    "            try:\n",
    "                feat_idx = feature_cols.index(feat_name)\n",
    "                if feat_idx < len(feature_cols):\n",
    "                    feature_importance[feat_idx] = weight\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        lime_attrs.append(feature_importance)\n",
    "\n",
    "    lime_attrs = np.array(lime_attrs)\n",
    "    print(f\"LIME attributions shape: {lime_attrs.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"LIME failed: {e}\")\n",
    "    lime_attrs = None\n",
    "\n",
    "# 2. SHAP\n",
    "print(\"\\n2. Computing SHAP explanations...\")\n",
    "try:\n",
    "    import shap\n",
    "\n",
    "    background = X_train[:100]\n",
    "\n",
    "    def model_predict_np(x):\n",
    "        arr = np.array(x)\n",
    "        if arr.ndim == 2:\n",
    "            arr = arr.reshape((arr.shape[0], 1, arr.shape[1]))\n",
    "        with torch.no_grad():\n",
    "            inp = torch.tensor(arr, dtype=torch.float32).to(device)\n",
    "            out = model(inp)\n",
    "            probs = torch.softmax(out, dim=1).cpu().numpy()\n",
    "        return probs\n",
    "\n",
    "    # Use KernelExplainer (slower but more compatible)\n",
    "    explainer = shap.KernelExplainer(\n",
    "        model_predict_np,\n",
    "        background.reshape(background.shape[0], -1)\n",
    "    )\n",
    "\n",
    "    shap_vals = explainer.shap_values(\n",
    "        test_subset_X[:NUM_SAMPLES].reshape(NUM_SAMPLES, -1),\n",
    "        nsamples=100\n",
    "    )\n",
    "\n",
    "    # Extract for class 1 (important)\n",
    "    if isinstance(shap_vals, list):\n",
    "        shap_attrs = np.array(shap_vals[1])\n",
    "    else:\n",
    "        shap_attrs = shap_vals\n",
    "    \n",
    "    # Handle multi-dimensional SHAP output\n",
    "    print(f\"SHAP raw output shape: {shap_attrs.shape}\")\n",
    "    \n",
    "    # If shape is (samples, features, classes), extract class 1\n",
    "    if shap_attrs.ndim == 3 and shap_attrs.shape[2] == 2:\n",
    "        print(\"Extracting SHAP attributions for class 1 (important)\")\n",
    "        shap_attrs = shap_attrs[:, :, 1]  # Take class 1 attributions\n",
    "    elif shap_attrs.ndim == 3 and shap_attrs.shape[1] != NUM_FEATURES:\n",
    "        # If middle dimension doesn't match features, might be transposed\n",
    "        print(\"Reshaping SHAP attributions\")\n",
    "        if shap_attrs.shape[2] == NUM_FEATURES:\n",
    "            shap_attrs = shap_attrs[:, 1, :]  # Take class 1, all features\n",
    "        else:\n",
    "            shap_attrs = shap_attrs[:, :, 1]  # Default: take last dimension as class\n",
    "\n",
    "    print(f\"SHAP attributions shape after processing: {shap_attrs.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"SHAP failed: {e}\")\n",
    "    shap_attrs = None\n",
    "\n",
    "# 3. Integrated Gradients\n",
    "print(\"\\n3. Computing Integrated Gradients explanations...\")\n",
    "try:\n",
    "    from captum.attr import IntegratedGradients\n",
    "\n",
    "    ig = IntegratedGradients(model)\n",
    "    X_test_t = torch.tensor(test_subset_X, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    baseline = torch.zeros_like(X_test_t[0:1])\n",
    "\n",
    "    ig_attrs = []\n",
    "    for i in tqdm(range(NUM_SAMPLES), desc=\"IG\"):\n",
    "        x = X_test_t[i:i+1]\n",
    "        target = int(test_subset_y[i])\n",
    "\n",
    "        attr = ig.attribute(x, baselines=baseline, target=target, n_steps=50)\n",
    "        ig_attrs.append(attr.squeeze().detach().cpu().numpy())\n",
    "\n",
    "    ig_attrs = np.array(ig_attrs)\n",
    "    print(f\"IG attributions shape: {ig_attrs.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"IG failed: {e}\")\n",
    "    ig_attrs = None\n",
    "\n",
    "# 4. DeepLIFT\n",
    "print(\"\\n4. Computing DeepLIFT explanations...\")\n",
    "try:\n",
    "    from captum.attr import DeepLift\n",
    "\n",
    "    dl = DeepLift(model)\n",
    "    X_test_t = torch.tensor(test_subset_X, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    baseline = torch.zeros_like(X_test_t[0:1])\n",
    "\n",
    "    dl_attrs = []\n",
    "    for i in tqdm(range(NUM_SAMPLES), desc=\"DeepLIFT\"):\n",
    "        x = X_test_t[i:i+1]\n",
    "        target = int(test_subset_y[i])\n",
    "\n",
    "        attr = dl.attribute(x, baselines=baseline, target=target)\n",
    "        dl_attrs.append(attr.squeeze().detach().cpu().numpy())\n",
    "\n",
    "    dl_attrs = np.array(dl_attrs)\n",
    "    print(f\"DeepLIFT attributions shape: {dl_attrs.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"DeepLIFT failed: {e}\")\n",
    "    dl_attrs = None\n",
    "\n",
    "# ==================== EVALUATE ALL EXPLAINERS ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ALL EXPLAINERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "\n",
    "explainers = {\n",
    "    'LIME': lime_attrs,\n",
    "    'SHAP': shap_attrs,\n",
    "    'IG': ig_attrs,\n",
    "    'DeepLIFT': dl_attrs\n",
    "}\n",
    "\n",
    "for explainer_name, attributions in explainers.items():\n",
    "    if attributions is None:\n",
    "        print(f\"\\nSkipping {explainer_name} (failed)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nEvaluating {explainer_name}...\")\n",
    "    print(f\"  Attribution shape before normalization: {attributions.shape}\")\n",
    "\n",
    "    # Faithfulness\n",
    "    print(f\"  Computing faithfulness metrics...\")\n",
    "    faith_corr_mean, faith_corr_std = compute_faithfulness_correlation(\n",
    "        model, test_subset_X, attributions\n",
    "    )\n",
    "    monotonicity = compute_monotonicity(model, test_subset_X, attributions)\n",
    "\n",
    "    # Robustness\n",
    "    print(f\"  Computing robustness metrics...\")\n",
    "    max_sens_mean, max_sens_std = compute_max_sensitivity(\n",
    "        test_subset_X, attributions\n",
    "    )\n",
    "\n",
    "    # Complexity\n",
    "    print(f\"  Computing complexity metrics...\")\n",
    "    complexity_mean, complexity_std = compute_low_complexity(attributions)\n",
    "\n",
    "    # Reliability\n",
    "    print(f\"  Computing reliability metrics...\")\n",
    "    rma_mean, rma_std = compute_RMA(attributions, gt_indices)\n",
    "    rra_mean, rra_std = compute_RRA(attributions, gt_indices)\n",
    "\n",
    "    results.append({\n",
    "        'Explainer': explainer_name,\n",
    "        'Faithfulness_Corr_Mean': faith_corr_mean,\n",
    "        'Faithfulness_Corr_Std': faith_corr_std,\n",
    "        'Monotonicity': monotonicity,\n",
    "        'MaxSensitivity_Mean': max_sens_mean,\n",
    "        'MaxSensitivity_Std': max_sens_std,\n",
    "        'Complexity_Mean': complexity_mean,\n",
    "        'Complexity_Std': complexity_std,\n",
    "        'RMA_Mean': rma_mean,\n",
    "        'RMA_Std': rma_std,\n",
    "        'RRA_Mean': rra_mean,\n",
    "        'RRA_Std': rra_std\n",
    "    })\n",
    "\n",
    "    print(f\"  âœ“ {explainer_name} complete\")\n",
    "\n",
    "# ==================== SAVE RESULTS ====================\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('xai_comparison_results.csv', index=False)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SAVED TO: xai_comparison_results.csv\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "# ==================== VISUALIZATION ====================\n",
    "print(\"\\nCreating comparison plots...\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('XAI Methods Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Faithfulness Correlation\n",
    "ax = axes[0, 0]\n",
    "ax.bar(results_df['Explainer'], results_df['Faithfulness_Corr_Mean'],\n",
    "       yerr=results_df['Faithfulness_Corr_Std'], capsize=5)\n",
    "ax.set_title('Faithfulness Correlation\\n(Higher is Better)')\n",
    "ax.set_ylabel('Correlation')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Monotonicity\n",
    "ax = axes[0, 1]\n",
    "ax.bar(results_df['Explainer'], results_df['Monotonicity'] * 100)\n",
    "ax.set_title('Monotonicity\\n(Higher is Better)')\n",
    "ax.set_ylabel('Percentage (%)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Max Sensitivity\n",
    "ax = axes[0, 2]\n",
    "ax.bar(results_df['Explainer'], results_df['MaxSensitivity_Mean'],\n",
    "       yerr=results_df['MaxSensitivity_Std'], capsize=5)\n",
    "ax.set_title('Max Sensitivity\\n(Lower is Better)')\n",
    "ax.set_ylabel('Sensitivity')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Complexity\n",
    "ax = axes[1, 0]\n",
    "ax.bar(results_df['Explainer'], results_df['Complexity_Mean'],\n",
    "       yerr=results_df['Complexity_Std'], capsize=5)\n",
    "ax.set_title('Complexity (Entropy)\\n(Lower is Better)')\n",
    "ax.set_ylabel('Entropy')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Relevance Mass Accuracy\n",
    "ax = axes[1, 1]\n",
    "ax.bar(results_df['Explainer'], results_df['RMA_Mean'],\n",
    "       yerr=results_df['RMA_Std'], capsize=5)\n",
    "ax.set_title('Relevance Mass Accuracy\\n(Higher is Better)')\n",
    "ax.set_ylabel('RMA')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Relevance Rank Accuracy\n",
    "ax = axes[1, 2]\n",
    "ax.bar(results_df['Explainer'], results_df['RRA_Mean'],\n",
    "       yerr=results_df['RRA_Std'], capsize=5)\n",
    "ax.set_title('Relevance Rank Accuracy\\n(Higher is Better)')\n",
    "ax.set_ylabel('RRA')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('xai_comparison_plots.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nPlots saved to: xai_comparison_plots.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest performers:\")\n",
    "print(f\"  Faithfulness: {results_df.loc[results_df['Faithfulness_Corr_Mean'].idxmax(), 'Explainer']}\")\n",
    "print(f\"  Monotonicity: {results_df.loc[results_df['Monotonicity'].idxmax(), 'Explainer']}\")\n",
    "print(f\"  Robustness: {results_df.loc[results_df['MaxSensitivity_Mean'].idxmin(), 'Explainer']}\")\n",
    "print(f\"  Complexity: {results_df.loc[results_df['Complexity_Mean'].idxmin(), 'Explainer']}\")\n",
    "print(f\"  RMA: {results_df.loc[results_df['RMA_Mean'].idxmax(), 'Explainer']}\")\n",
    "print(f\"  RRA: {results_df.loc[results_df['RRA_Mean'].idxmax(), 'Explainer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceffa31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
