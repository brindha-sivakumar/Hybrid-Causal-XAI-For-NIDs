{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de7c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Causal Discovery for NIDS Alert Classification\n",
    "# Implements PC and Hill-Climb algorithms to learn causal relationships\n",
    "# Optimized for M1 Mac\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 2: CAUSAL DISCOVERY FOR NIDS ALERTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "DATA_PATH = '../UNSW_NB15_training-set.csv'\n",
    "\n",
    "# Ensure label column exists\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "if 'label' not in df.columns and 'Label' not in df.columns:\n",
    "    if 'attack_cat' in df.columns:\n",
    "        df['label'] = (df['attack_cat'] != 'Normal').astype(int)\n",
    "        print(\"✓ Created binary label from attack_cat\")\n",
    "\n",
    "TARGET = 'label' if 'label' in df.columns else 'Label'\n",
    "#DATA_PATH = '../dataset-labeled-anon-ip.csv'\n",
    "SAMPLE_SIZE = 2000  # Per class (10K total)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ==================== SELECT RELEVANT COLUMNS ====================\n",
    "# Now continue with your feature selection...\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Feature selection based on:\n",
    "# 1. SOC expert features (from paper)\n",
    "# 2. Top XAI features (from your Step 1 SHAP analysis)\n",
    "SOC_FEATURES = [\n",
    "    'SignatureMatchesPerDay',\n",
    "    'Similarity', \n",
    "    'SCAS',\n",
    "    'SignatureID',\n",
    "    'SignatureIDSimilarity'\n",
    "]\n",
    "\n",
    "# Add top features from network traffic analysis\n",
    "ADDITIONAL_FEATURES = [\n",
    "    'Proto',\n",
    "    'AlertCount',\n",
    "    'IntPort',\n",
    "    'ExtPort',\n",
    "    'ProtoSimilarity'\n",
    "]\n",
    "\n",
    "SELECTED_FEATURES = SOC_FEATURES + ADDITIONAL_FEATURES\n",
    "TARGET = 'Label'\n",
    "\n",
    "print(f\"\\nSelected {len(SELECTED_FEATURES)} features for causal discovery:\")\n",
    "for i, f in enumerate(SELECTED_FEATURES, 1):\n",
    "    print(f\"  {i}. {f}\")\n",
    "\n",
    "\"\"\"\n",
    "# ==================== FEATURE SELECTION ====================\n",
    "# Use your validated ground truth features\n",
    "SOC_FEATURES = [\n",
    "    'proto',           # Protocol - #1 in explainers\n",
    "    'sttl',            # Source TTL - #2 \n",
    "    'state',           # Connection state - #3\n",
    "    'dtcpb',           # TCP dest base seq - Top 5\n",
    "    'is_sm_ips_ports'  # Port scan indicator - #3 in LIME\n",
    "]\n",
    "\n",
    "# Add related features for causal chains\n",
    "ADDITIONAL_FEATURES = [\n",
    "    'dttl',            # Dest TTL (related to sttl)\n",
    "    'stcpb',           # TCP source base seq (related to dtcpb)\n",
    "    'service',         # Service (related to proto)\n",
    "    'dwin',            # Dest window (TCP behavior)\n",
    "    'swin'             # Source window (TCP behavior)\n",
    "]\n",
    "\n",
    "SELECTED_FEATURES = SOC_FEATURES + ADDITIONAL_FEATURES\n",
    "TARGET = 'label'  # or 'Label' depending on your dataset\n",
    "\n",
    "required_cols = SELECTED_FEATURES + [TARGET]\n",
    "df = df[required_cols].copy()\n",
    "\n",
    "print(f\"\\nSelected {len(SELECTED_FEATURES)} features for causal discovery:\")\n",
    "for i, f in enumerate(SELECTED_FEATURES, 1):\n",
    "    print(f\"  {i}. {f}\")\n",
    "    \n",
    "# ==================== DATA LOADING & PREPROCESSING ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING AND PREPROCESSING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if file exists\n",
    "if not Path(DATA_PATH).exists():\n",
    "    print(f\"ERROR: {DATA_PATH} not found in current directory\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(\"\\nPlease ensure the dataset file is in the same directory as this script.\")\n",
    "    raise FileNotFoundError(f\"{DATA_PATH} not found\")\n",
    "\n",
    "# Get file info\n",
    "file_size = Path(DATA_PATH).stat().st_size / (1024**2)\n",
    "print(f\"Found {DATA_PATH} ({file_size:.2f} MB)\")\n",
    "\n",
    "# Load dataset with robust error handling\n",
    "print(f\"Loading data...\")\n",
    "try:\n",
    "    # Method 1: Python engine with line limit (most reliable for macOS)\n",
    "    df = pd.read_csv(DATA_PATH, \n",
    "                     engine='python',\n",
    "                     encoding='utf-8',\n",
    "                     on_bad_lines='skip')  # Skip problematic lines\n",
    "    print(f\"✓ Loaded dataset: {df.shape}\")\n",
    "    \n",
    "except Exception as e1:\n",
    "    print(f\"Method 1 failed: {e1}\")\n",
    "    try:\n",
    "        # Method 2: Read first N lines that we need\n",
    "        print(\"Trying to read specific number of rows...\")\n",
    "        # We only need ~10K samples anyway\n",
    "        df = pd.read_csv(DATA_PATH, \n",
    "                         nrows=50000,  # Read first 50K rows\n",
    "                         engine='python')\n",
    "        print(f\"✓ Loaded subset: {df.shape}\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"Method 2 failed: {e2}\")\n",
    "        print(\"\\nTrying to diagnose the file...\")\n",
    "        \n",
    "        # Check if file is readable\n",
    "        try:\n",
    "            with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "                first_line = f.readline()\n",
    "                print(f\"File is readable. Header: {first_line[:100]}\")\n",
    "        except Exception as e3:\n",
    "            print(f\"Cannot read file: {e3}\")\n",
    "            raise\n",
    "        \n",
    "        raise Exception(\"All loading methods failed. File may be corrupted.\")\n",
    "\n",
    "\n",
    "# ==================== HANDLE LABEL COLUMN ====================\n",
    "if 'attack_cat' in df.columns and 'label' not in df.columns:\n",
    "    df['label'] = (df['attack_cat'] != 'Normal').astype(int)\n",
    "    print(f\"✓ Created binary label from attack_cat\")\n",
    "    print(f\"  Normal (0): {(df['label']==0).sum()}\")\n",
    "    print(f\"  Attack (1): {(df['label']==1).sum()}\")\n",
    "\n",
    "# Determine target column\n",
    "if 'label' in df.columns:\n",
    "    TARGET = 'label'\n",
    "elif 'Label' in df.columns:\n",
    "    TARGET = 'Label'\n",
    "else:\n",
    "    raise ValueError(\"No label column found!\")\n",
    "\n",
    "print(f\"✓ Using target column: {TARGET}\")\n",
    "\n",
    "# ==================== ENCODE CATEGORICAL FEATURES ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENCODING CATEGORICAL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Identify categorical columns that we're using\n",
    "categorical_cols = ['proto', 'service', 'state']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nProcessing {col}...\")\n",
    "        \n",
    "        # Check data type\n",
    "        print(f\"  Original dtype: {df[col].dtype}\")\n",
    "        print(f\"  Sample values: {df[col].head().tolist()}\")\n",
    "        \n",
    "        # If already numeric, skip\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            print(f\"  ✓ Already numeric, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Encode string values\n",
    "        le = LabelEncoder()\n",
    "        df[col] = df[col].fillna('unknown')  # Handle NaN\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        \n",
    "        print(f\"  ✓ Encoded: {len(le.classes_)} categories\")\n",
    "        print(f\"  Categories: {list(le.classes_)[:10]}\")  # Show first 10\n",
    "        print(f\"  New values: {df[col].unique()[:10]}\")\n",
    "\n",
    "# Save encoders\n",
    "if label_encoders:\n",
    "    joblib.dump(label_encoders, 'label_encoders_causal.joblib')\n",
    "    print(f\"\\n✓ Saved {len(label_encoders)} label encoders\")\n",
    "\n",
    "# ==================== SELECT RELEVANT COLUMNS ====================\n",
    "# Verify all features exist\n",
    "missing_features = [f for f in SELECTED_FEATURES if f not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"\\n⚠ WARNING: Missing features: {missing_features}\")\n",
    "    SELECTED_FEATURES = [f for f in SELECTED_FEATURES if f in df.columns]\n",
    "    print(f\"  Using {len(SELECTED_FEATURES)} available features\")\n",
    "\n",
    "required_cols = SELECTED_FEATURES + [TARGET]\n",
    "df = df[required_cols].copy()\n",
    "print(f\"\\n✓ Selected {len(required_cols)} columns\")\n",
    "\n",
    "# ==================== HANDLE MISSING VALUES ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(f\"Missing values detected:\")\n",
    "    for col, count in missing_counts[missing_counts > 0].items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  {col}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Fill with median for numeric, mode for categorical\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            if df[col].dtype in ['int64', 'float64']:\n",
    "                fill_value = df[col].median()\n",
    "                df[col] = df[col].fillna(fill_value)\n",
    "                print(f\"  ✓ Filled {col} with median: {fill_value}\")\n",
    "            else:\n",
    "                fill_value = df[col].mode()[0]\n",
    "                df[col] = df[col].fillna(fill_value)\n",
    "                print(f\"  ✓ Filled {col} with mode: {fill_value}\")\n",
    "else:\n",
    "    print(\"✓ No missing values\")\n",
    "\n",
    "# ==================== BALANCE CLASSES ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BALANCING CLASSES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"Original distribution:\")\n",
    "print(df[TARGET].value_counts())\n",
    "\n",
    "df_balanced = pd.concat([\n",
    "    df[df[TARGET] == 0].sample(n=min(SAMPLE_SIZE, len(df[df[TARGET] == 0])), random_state=SEED),\n",
    "    df[df[TARGET] == 1].sample(n=min(SAMPLE_SIZE, len(df[df[TARGET] == 1])), random_state=SEED)\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nBalanced dataset: {df_balanced.shape}\")\n",
    "print(f\"Balanced distribution:\")\n",
    "print(df_balanced[TARGET].value_counts())\n",
    "\n",
    "# ==================== DISCRETIZATION ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISCRETIZING CONTINUOUS FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Causal discovery works better with discrete/categorical data\n",
    "# Using quantile-based binning (5 bins)\n",
    "df_discrete = df_balanced.copy()\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "\n",
    "continuous_features = [f for f in SELECTED_FEATURES \n",
    "                       if df_discrete[f].nunique() > 10]\n",
    "\n",
    "print(f\"Discretizing {len(continuous_features)} continuous features:\")\n",
    "for f in continuous_features:\n",
    "    unique_before = df_discrete[f].nunique()\n",
    "    df_discrete[f] = discretizer.fit_transform(df_discrete[[f]])\n",
    "    unique_after = df_discrete[f].nunique()\n",
    "    print(f\"  {f}: {unique_before} → {unique_after} bins\")\n",
    "\n",
    "# Save preprocessed data\n",
    "df_discrete.to_csv('causal_discovery_data.csv', index=False)\n",
    "print(\"\\nSaved preprocessed data to: causal_discovery_data.csv\")\n",
    "\n",
    "# ==================== DOMAIN KNOWLEDGE CONSTRAINTS ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPLYING DOMAIN KNOWLEDGE CONSTRAINTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define forbidden edges based on network security domain knowledge\n",
    "# Format: (from, to) - \"from cannot cause to\"\n",
    "\"\"\"\n",
    "FORBIDDEN_EDGES = [\n",
    "    # Low-level protocol features cannot cause high-level signature features\n",
    "    ('Proto', 'SignatureID'),\n",
    "    ('Proto', 'SignatureMatchesPerDay'),\n",
    "    ('ExtPort', 'SignatureID'),\n",
    "    ('IntPort', 'SignatureID'),\n",
    "    \n",
    "    # Signature ID is determined by signature rules, not by similarity\n",
    "    ('Similarity', 'SignatureID'),\n",
    "    ('SCAS', 'SignatureID'),\n",
    "    \n",
    "    # Port numbers don't cause protocol\n",
    "    ('IntPort', 'Proto'),\n",
    "    ('ExtPort', 'Proto'),\n",
    "    \n",
    "    # Alert count is an effect, not a cause of individual features\n",
    "    ('AlertCount', 'Proto'),\n",
    "    ('AlertCount', 'IntPort'),\n",
    "    ('AlertCount', 'ExtPort'),\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(FORBIDDEN_EDGES)} forbidden edges:\")\n",
    "for i, (src, dst) in enumerate(FORBIDDEN_EDGES[:5], 1):\n",
    "    print(f\"  {i}. {src} → {dst} (forbidden)\")\n",
    "print(f\"  ... and {len(FORBIDDEN_EDGES) - 5} more\")\n",
    "\"\"\"\n",
    "\n",
    "FORBIDDEN_EDGES = [\n",
    "    # Protocol is fundamental - nothing causes it\n",
    "    ('sttl', 'proto'),\n",
    "    ('dttl', 'proto'),\n",
    "    ('state', 'proto'),\n",
    "    ('dtcpb', 'proto'),\n",
    "    ('stcpb', 'proto'),\n",
    "    ('service', 'proto'),\n",
    "    \n",
    "    # TTL values are set at packet creation\n",
    "    ('state', 'sttl'),\n",
    "    ('state', 'dttl'),\n",
    "    ('dtcpb', 'sttl'),\n",
    "    ('dtcpb', 'dttl'),\n",
    "    \n",
    "    # TCP sequences don't cause state\n",
    "    ('dtcpb', 'state'),\n",
    "    ('stcpb', 'state'),\n",
    "    \n",
    "    # Window sizes are effects\n",
    "    ('dwin', 'proto'),\n",
    "    ('swin', 'proto'),\n",
    "    ('dwin', 'state'),\n",
    "    ('swin', 'state'),\n",
    "    \n",
    "    # Scanning indicator is behavioral outcome\n",
    "    ('is_sm_ips_ports', 'proto'),\n",
    "    ('is_sm_ips_ports', 'sttl'),\n",
    "    ('is_sm_ips_ports', 'dttl'),\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(FORBIDDEN_EDGES)} forbidden edges\")\n",
    "\n",
    "# ==================== INSTALL CAUSAL-LEARN ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECKING CAUSAL-LEARN INSTALLATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    #import causal_learn\n",
    "    print(\"causal-learn is installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing causal-learn...\")\n",
    "    os.system(\"pip install causal-learn\")\n",
    "    #import causal_learn\n",
    "\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.search.ScoreBased.GES import ges\n",
    "from causallearn.utils.GraphUtils import GraphUtils\n",
    "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge\n",
    "from causallearn.graph.GraphNode import GraphNode\n",
    "\n",
    "# ==================== PC ALGORITHM ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING PC ALGORITHM (Constraint-Based)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare data matrix\n",
    "X = df_discrete[SELECTED_FEATURES].values\n",
    "feature_names = SELECTED_FEATURES\n",
    "\n",
    "# Set up background knowledge (forbidden edges)\n",
    "bk = BackgroundKnowledge()\n",
    "\n",
    "# Create GraphNode objects for each feature\n",
    "graph_nodes = [GraphNode(f) for f in feature_names]\n",
    "\n",
    "# Add forbidden edges using GraphNode objects\n",
    "for src, dst in FORBIDDEN_EDGES:\n",
    "    if src in feature_names and dst in feature_names:\n",
    "        src_idx = feature_names.index(src)\n",
    "        dst_idx = feature_names.index(dst)\n",
    "        bk.add_forbidden_by_node(graph_nodes[src_idx], graph_nodes[dst_idx])\n",
    "\n",
    "print(\"Running PC algorithm...\")\n",
    "print(f\"  - Data shape: {X.shape}\")\n",
    "print(f\"  - Independence test: fisherz\")\n",
    "print(f\"  - Significance level: α = 0.05\")\n",
    "print(f\"  - Background knowledge: {len(FORBIDDEN_EDGES)} forbidden edges\")\n",
    "\n",
    "# Run PC algorithm\n",
    "cg_pc = pc(\n",
    "    X,\n",
    "    alpha=0.05,\n",
    "    indep_test='fisherz',\n",
    "    background_knowledge=bk,\n",
    "    verbose=False,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "print(\"\\nPC Algorithm completed!\")\n",
    "\n",
    "# Extract graph\n",
    "pc_graph = cg_pc.G\n",
    "pc_edges = []\n",
    "for i in range(len(feature_names)):\n",
    "    for j in range(len(feature_names)):\n",
    "        if pc_graph.graph[i, j] == 1:  # i -> j\n",
    "            pc_edges.append((feature_names[i], feature_names[j]))\n",
    "        elif pc_graph.graph[i, j] == -1 and pc_graph.graph[j, i] == 1:  # i <- j\n",
    "            pc_edges.append((feature_names[j], feature_names[i]))\n",
    "\n",
    "print(f\"Discovered {len(pc_edges)} directed edges\")\n",
    "\n",
    "# ==================== HILL-CLIMB (GES) ALGORITHM ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING GES ALGORITHM (Score-Based, similar to Hill-Climb)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Running GES algorithm...\")\n",
    "print(f\"  - Scoring method: BIC\")\n",
    "print(f\"  - Data shape: {X.shape}\")\n",
    "\n",
    "# Run GES (Greedy Equivalence Search - score-based like Hill-Climb)\n",
    "record_ges = ges(X, score_func='local_score_BIC', maxP=None)\n",
    "\n",
    "print(\"\\nGES Algorithm completed!\")\n",
    "\n",
    "# Extract graph\n",
    "ges_graph = record_ges['G']\n",
    "ges_edges = []\n",
    "for i in range(len(feature_names)):\n",
    "    for j in range(len(feature_names)):\n",
    "        if ges_graph.graph[i, j] == 1 and ges_graph.graph[j, i] == -1:  # i -> j\n",
    "            ges_edges.append((feature_names[i], feature_names[j]))\n",
    "\n",
    "print(f\"Discovered {len(ges_edges)} directed edges\")\n",
    "\n",
    "# ==================== COMPARISON & CONSENSUS ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARING PC vs GES RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pc_set = set(pc_edges)\n",
    "ges_set = set(ges_edges)\n",
    "\n",
    "consensus_edges = pc_set.intersection(ges_set)\n",
    "pc_only = pc_set - ges_set\n",
    "ges_only = ges_set - pc_set\n",
    "\n",
    "print(f\"\\nConsensus edges (both algorithms agree): {len(consensus_edges)}\")\n",
    "print(f\"PC-only edges: {len(pc_only)}\")\n",
    "print(f\"GES-only edges: {len(ges_only)}\")\n",
    "\n",
    "print(\"\\nConsensus edges:\")\n",
    "for src, dst in sorted(consensus_edges):\n",
    "    print(f\"  {src} → {dst}\")\n",
    "\n",
    "# ==================== BUILD FINAL CAUSAL GRAPH ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUILDING FINAL CAUSAL GRAPH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use consensus + high-confidence edges from both\n",
    "final_edges = list(consensus_edges)\n",
    "\n",
    "# Add edges that appear in at least one algorithm and make domain sense\n",
    "candidate_edges = pc_set.union(ges_set) - consensus_edges\n",
    "\n",
    "print(f\"\\nFinal causal graph contains {len(final_edges)} edges\")\n",
    "\n",
    "# Create NetworkX directed graph\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(SELECTED_FEATURES)\n",
    "G.add_edges_from(final_edges)\n",
    "\n",
    "# Add outcome node\n",
    "G.add_node(TARGET)\n",
    "\n",
    "# Check which features are connected to the outcome\n",
    "print(f\"\\nAnalyzing causal relationships with outcome ({TARGET})...\")\n",
    "\n",
    "# Simple correlation analysis with outcome\n",
    "correlations = []\n",
    "for feature in SELECTED_FEATURES:\n",
    "    corr, pval = stats.spearmanr(df_discrete[feature], df_discrete[TARGET])\n",
    "    correlations.append((feature, corr, pval))\n",
    "\n",
    "# Add edges to outcome for strong correlations\n",
    "for feature, corr, pval in sorted(correlations, key=lambda x: abs(x[1]), reverse=True):\n",
    "    if pval < 0.01 and abs(corr) > 0.1:\n",
    "        G.add_edge(feature, TARGET)\n",
    "        print(f\"  {feature} → {TARGET} (ρ={corr:.3f}, p={pval:.2e})\")\n",
    "\n",
    "# ==================== GRAPH ANALYSIS ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CAUSAL GRAPH ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nNodes: {G.number_of_nodes()}\")\n",
    "print(f\"Edges: {G.number_of_edges()}\")\n",
    "\n",
    "# Find root causes (nodes with no incoming edges)\n",
    "root_causes = [node for node in G.nodes() if G.in_degree(node) == 0]\n",
    "print(f\"\\nRoot causes (no incoming edges): {len(root_causes)}\")\n",
    "for rc in root_causes:\n",
    "    print(f\"  - {rc}\")\n",
    "\n",
    "# Find direct causes of outcome\n",
    "if TARGET in G:\n",
    "    direct_causes = list(G.predecessors(TARGET))\n",
    "    print(f\"\\nDirect causes of {TARGET}: {len(direct_causes)}\")\n",
    "    for dc in direct_causes:\n",
    "        print(f\"  - {dc}\")\n",
    "\n",
    "# Find features with highest causal influence (most outgoing edges)\n",
    "influence = [(node, G.out_degree(node)) for node in SELECTED_FEATURES]\n",
    "influence.sort(key=lambda x: x[1], reverse=True)\n",
    "print(f\"\\nMost influential features (highest out-degree):\")\n",
    "for node, degree in influence[:5]:\n",
    "    print(f\"  - {node}: {degree} outgoing edges\")\n",
    "\n",
    "# ==================== VISUALIZATION ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# PC Algorithm Graph\n",
    "ax1 = axes[0]\n",
    "G_pc = nx.DiGraph()\n",
    "G_pc.add_nodes_from(SELECTED_FEATURES)\n",
    "G_pc.add_edges_from(pc_edges)\n",
    "\n",
    "pos1 = nx.spring_layout(G_pc, k=2, iterations=50, seed=SEED)\n",
    "nx.draw_networkx_nodes(G_pc, pos1, node_color='lightblue', \n",
    "                       node_size=1500, alpha=0.9, ax=ax1)\n",
    "nx.draw_networkx_labels(G_pc, pos1, font_size=8, font_weight='bold', ax=ax1)\n",
    "nx.draw_networkx_edges(G_pc, pos1, edge_color='gray', \n",
    "                       arrows=True, arrowsize=20, ax=ax1)\n",
    "ax1.set_title(f'PC Algorithm\\n{len(pc_edges)} edges', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# GES Algorithm Graph\n",
    "ax2 = axes[1]\n",
    "G_ges = nx.DiGraph()\n",
    "G_ges.add_nodes_from(SELECTED_FEATURES)\n",
    "G_ges.add_edges_from(ges_edges)\n",
    "\n",
    "pos2 = nx.spring_layout(G_ges, k=2, iterations=50, seed=SEED)\n",
    "nx.draw_networkx_nodes(G_ges, pos2, node_color='lightcoral', \n",
    "                       node_size=1500, alpha=0.9, ax=ax2)\n",
    "nx.draw_networkx_labels(G_ges, pos2, font_size=8, font_weight='bold', ax=ax2)\n",
    "nx.draw_networkx_edges(G_ges, pos2, edge_color='gray', \n",
    "                       arrows=True, arrowsize=20, ax=ax2)\n",
    "ax2.set_title(f'GES Algorithm\\n{len(ges_edges)} edges', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('causal_graphs_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Saved: causal_graphs_comparison.png\")\n",
    "\n",
    "# Final consensus graph with outcome\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "\n",
    "# Color nodes by type\n",
    "node_colors = []\n",
    "for node in G.nodes():\n",
    "    if node == TARGET:\n",
    "        node_colors.append('gold')\n",
    "    elif node in SOC_FEATURES:\n",
    "        node_colors.append('lightblue')\n",
    "    else:\n",
    "        node_colors.append('lightgreen')\n",
    "\n",
    "pos = nx.spring_layout(G, k=3, iterations=100, seed=SEED)\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                       node_size=2000, alpha=0.9, ax=ax)\n",
    "nx.draw_networkx_labels(G, pos, font_size=9, font_weight='bold', ax=ax)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='gray', \n",
    "                       arrows=True, arrowsize=20, \n",
    "                       connectionstyle='arc3,rad=0.1', ax=ax)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='gold', label='Outcome'),\n",
    "    Patch(facecolor='lightblue', label='SOC Expert Features'),\n",
    "    Patch(facecolor='lightgreen', label='Additional Features')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "ax.set_title(f'Final Causal Graph\\n{G.number_of_edges()} edges', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_causal_graph.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Saved: final_causal_graph.png\")\n",
    "\n",
    "# ==================== SAVE RESULTS ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save edge lists\n",
    "pd.DataFrame(pc_edges, columns=['Source', 'Target']).to_csv(\n",
    "    'pc_edges.csv', index=False\n",
    ")\n",
    "pd.DataFrame(ges_edges, columns=['Source', 'Target']).to_csv(\n",
    "    'ges_edges.csv', index=False\n",
    ")\n",
    "pd.DataFrame(final_edges, columns=['Source', 'Target']).to_csv(\n",
    "    'final_causal_edges.csv', index=False\n",
    ")\n",
    "\n",
    "# Save graph as adjacency matrix\n",
    "adj_matrix = nx.to_pandas_adjacency(G, dtype=int)\n",
    "adj_matrix.to_csv('causal_adjacency_matrix.csv')\n",
    "\n",
    "# Save NetworkX graph object\n",
    "#nx.write_gpickle(G, 'causal_graph.gpickle')\n",
    "\n",
    "# Save NetworkX graph object using pickle.dump()\n",
    "try:\n",
    "    with open('causal_graph.gpickle', 'wb') as f:\n",
    "        pickle.dump(G, f)\n",
    "    print(\"✓ Saved NetworkX graph object using pickle.dump()\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving gpickle: {e}\")\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - causal_discovery_data.csv (preprocessed data)\")\n",
    "print(\"  - pc_edges.csv (PC algorithm results)\")\n",
    "print(\"  - ges_edges.csv (GES algorithm results)\")\n",
    "print(\"  - final_causal_edges.csv (consensus edges)\")\n",
    "print(\"  - causal_adjacency_matrix.csv (adjacency matrix)\")\n",
    "print(\"  - causal_graph.gpickle (NetworkX graph object)\")\n",
    "print(\"  - causal_graphs_comparison.png (visualization)\")\n",
    "print(\"  - final_causal_graph.png (final graph visualization)\")\n",
    "\n",
    "# ==================== CAUSAL QUERY INTERFACE ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CAUSAL QUERY INTERFACE (for Step 4 integration)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def find_root_causes(graph, target_feature):\n",
    "    \"\"\"Find all ancestors (root causes) of a target feature\"\"\"\n",
    "    if target_feature not in graph:\n",
    "        return []\n",
    "    ancestors = nx.ancestors(graph, target_feature)\n",
    "    return list(ancestors)\n",
    "\n",
    "def find_causal_path(graph, source, target):\n",
    "    \"\"\"Find causal path from source to target\"\"\"\n",
    "    if source not in graph or target not in graph:\n",
    "        return None\n",
    "    try:\n",
    "        path = nx.shortest_path(graph, source, target)\n",
    "        return path\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "def get_direct_causes(graph, feature):\n",
    "    \"\"\"Get direct causes (parents) of a feature\"\"\"\n",
    "    if feature not in graph:\n",
    "        return []\n",
    "    return list(graph.predecessors(feature))\n",
    "\n",
    "def get_direct_effects(graph, feature):\n",
    "    \"\"\"Get direct effects (children) of a feature\"\"\"\n",
    "    if feature not in graph:\n",
    "        return []\n",
    "    return list(graph.successors(feature))\n",
    "\n",
    "# Example queries\n",
    "print(\"\\nExample Causal Queries:\")\n",
    "\n",
    "# Query 1: Root causes of SCAS\n",
    "if 'SCAS' in G:\n",
    "    root_causes_scas = find_root_causes(G, 'SCAS')\n",
    "    print(f\"\\n1. Root causes of SCAS: {root_causes_scas}\")\n",
    "\n",
    "# Query 2: Direct causes of Label (important alerts)\n",
    "if TARGET in G:\n",
    "    direct_causes_label = get_direct_causes(G, TARGET)\n",
    "    print(f\"\\n2. Direct causes of {TARGET}: {direct_causes_label}\")\n",
    "\n",
    "# Query 3: Causal path from SignatureMatchesPerDay to Label\n",
    "if 'SignatureMatchesPerDay' in G and TARGET in G:\n",
    "    path = find_causal_path(G, 'SignatureMatchesPerDay', TARGET)\n",
    "    if path:\n",
    "        print(f\"\\n3. Causal path SignatureMatchesPerDay → {TARGET}:\")\n",
    "        print(f\"   {' → '.join(path)}\")\n",
    "\n",
    "# Save query functions for Step 4\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2 COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Review causal graphs (causal_graphs_comparison.png)\")\n",
    "print(\"  2. Validate edges with domain experts\")\n",
    "print(\"  3. Proceed to Step 4: Hybrid Explanation Generation\")\n",
    "print(\"\\nTo use in Step 4:\")\n",
    "print(\"  import networkx as nx\")\n",
    "print(\"  G = nx.read_gpickle('causal_graph.gpickle')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
